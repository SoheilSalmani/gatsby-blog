---
title: Data Pipelines with Apache Airflow
resourceId: "9781617296901"
stoppedAt: Communicating with external systems
---

## Getting started

### Meet Apache Airflow

#### Introducing data pipelines

Data pipelines generally consist of several tasks or actions that need to be
executed to achieve the desired result. For example, say we want to build a
small weather dashboard that tells us what the weather will be like in the
coming week. To implement this live weather dashboard, we need to perform
something like the following steps:

1. Fetch weather forecast data from a weather API.
2. Clean or otherwise transform the fetched data (e.g., converting temperatures
   from Fahrenheit to Celsius or vice versa), so that the data suits our
   purpose.
3. Push the transformed data to the weather dashboard.

As such, we need to make sure that this implicit task order is also enforced
when running this data process.

##### Data pipelines as graphs

One way to make dependencies between tasks more explicit is to draw the data
pipeline as a _directed graph_.

<Figure src="/media/weather-data-pipeline-graph.png">
  Graph representation of the data pipeline for the weather dashboard. Nodes
  represent tasks and directed edges represent dependencies between tasks (with
  an edge pointing from task A to task B, indicating that task A needs to be run
  before task B).
</Figure>

This type of graph is typically called a _directed acyclic graph (DAG)_, as the
graph contains directed edges and does not contain any loops or cycles
(acyclic). This acyclic property is extremely important, as it prevents us from
running into circular dependencies between tasks (where task A depends on task B
and vice versa). This logical inconsistency leads to a deadlock type of
situation, in which neither task 2 nor 3 can run, preventing us from executing
the graph.

The acyclic property of DAGs is used by Airflow (and many other workflow
managers) to efficiently resolve and execute these graphs of tasks.

##### Executing a pipeline graph

A nice property of this DAG representation is that it provides a relatively
straightforward algorithm that we can use for running the pipeline.
Conceptually, this algorithm consists of the following steps:

1. For each open (= uncompleted) task in the graph, do the following: – For each
   edge pointing toward the task, check if the “upstream” task on the other end
   of the edge has been completed. – If all upstream tasks have been completed,
   add the task under consideration to a queue of tasks to be executed.
2. Execute the tasks in the execution queue, marking them completed once they
   finish performing their work.
3. Jump back to step 1 and repeat until all tasks in the graph have been
   completed.

##### Pipeline graphs vs. sequential scripts

To build a pipeline for training the ML model, we need to implement something
like the following steps:

1. Prepare the sales data by doing the following:
   - Fetching the sales data from the source system
   - Cleaning/transforming the sales data to fit requirements
2. Prepare the weather data by doing the following:
   - Fetching the weather forecast data from an API
   - Cleaning/transforming the weather data to fit requirements
3. Combine the sales and weather data sets to create the combined data set that
   can be used as input for creating a predictive ML model.
4. Train the ML model using the combined data set.
5. Deploy the ML model so that it can be used by the business.

<Figure src="/media/umbrella-company-data-pipeline.png">
  Overview of the umbrella demand use case, in which historical weather and
  sales data are used to train a model that predicts future sales demands
  depending on weather forecasts.
</Figure>

One important difference from our previous example is that the first steps of
this pipeline are in fact independent of each other, as they involve two
separate data sets. This is clearly illustrated by the two separate branches in
the graph representation of the pipeline, which can be executed in parallel if
we apply our graph execution algorithm, making better use of available resources
and potentially decreasing the running time of a pipeline compared to executing
the tasks sequentially.

Another useful property of the graph-based representation is that it clearly
separates pipelines into small incremental tasks rather than having one
monolithic script or process that does all the work. Although having a single
monolithic script may not initially seem like that much of a problem, it can
introduce some inefficiencies when tasks in the pipeline fail, as we would have
to rerun the entire script. In contrast, in the graph representation, we need
only to rerun any failing tasks (and any downstream dependencies).

##### Running pipeline using workflow managers

The challenge of running graphs of dependent tasks is hardly a new problem in
computing. Over the years, many so-called “workflow management” solutions have
been developed to tackle this problem, which generally allow you to define and
execute graphs of tasks as workflows or pipelines.

Although each of these workflow managers has its own strengths and weaknesses,
they all provide similar core functionality that allows you to define and run
pipelines containing multiple tasks with dependencies.

One of the key differences between these tools is how they define their
workflows. For example, tools such as Oozie use static (XML) files to define
workflows, which provides legible workflows but limited flexibility. Other
solutions such as Luigi and Airflow allow you to define workflows as code, which
provides greater flexibility but can be more challenging to read and test
(depending on the coding skills of the person implementing the workflow).

Other key differences lie in the extent of features provided by the workflow
manager. For example, tools such as Make and Luigi do not provide built-in
support for scheduling workflows, meaning that you’ll need an extra tool like
Cron if you want to run your workflow on a recurring schedule. Other tools may
provide extra functionality such as scheduling, monitoring, user-friendly web
interfaces, and so on built into the platform, meaning that you don’t have to
stitch together multiple tools yourself to get these features.

All in all, picking the right workflow management solution for your needs will
require some careful consideration of the key features of the different
solutions and how they fit your requirements.

#### Introducing Airflow

##### Defining pipelines flexibly in (Python) code

In Airflow, you define your DAGs using Python code in DAG files, which are
essentially Python scripts that describe the structure of the corresponding DAG.

One advantage of defining Airflow DAGs in Python code is that this programmatic
approach provides you with a lot of flexibility for building DAGs. For example,
as we will see later in this book, you can use Python code to dynamically
generate optional tasks depending on certain conditions or even generate entire
DAGs based on external metadata or configuration files. This flexibility gives a
great deal of customization in how you build your pipelines, allowing you to fit
Airflow to your needs for building arbitrarily complex pipelines.

<Figure src="/media/airflow-dag-file.png">
  Airflow pipelines are defined as DAGs using Python code in DAG files. Each DAG
  file typically defines one DAG, which describes the different tasks and their
  dependencies. Besides this, the DAG also defines a schedule interval that
  determines when the DAG is executed by Airflow.
</Figure>

##### Scheduling and executing pipelines

Airflow allows you to define a schedule interval for each DAG, which determines
exactly when your pipeline is run by Airflow. This way, you can tell Airflow to
execute your DAG every hour, every day, every week, and so on, or even use more
complicated schedule intervals based on Cron-like expressions.

At a high level, Airflow is organized into three main components:

- _The Airflow scheduler_—Parses DAGs, checks their schedule interval, and (if
  the DAGs’ schedule has passed) starts scheduling the DAGs’ tasks for execution
  by passing them to the Airflow workers.
- _The Airflow workers_—Pick up tasks that are scheduled for execution and
  execute them. As such, the workers are responsible for actually “doing the
  work.”
- _The Airflow webserver_—Visualizes the DAGs parsed by the scheduler and
  provides the main interface for users to monitor DAG runs and their results.

<Figure src="/media/airflow-scheduler-workers-webserver.png">
  Overview of the main components involved in Airflow (e.g., the Airflow
  webserver, scheduler, and workers).
</Figure>

<Figure src="/media/airflow-process-involved.png">
  Schematic overview of the process involved in developing and executing
  pipelines as DAGs using Airflow.
</Figure>

Once tasks have been queued for execution, they are picked up by a pool of
Airflow workers that execute tasks in parallel and track their results. These
results are communicated to Airflow’s metastore so that users can track the
progress of tasks and view their logs using the Airflow web interface (provided
by the Airflow webserver).

##### Monitoring and handling failures

<Figure src="/media/airflow-main-page.png">
  The main page of Airflow’s web interface, showing an overview of the available
  DAGs and their recent results.
</Figure>

<Figure src="/media/airflow-graph-view.png">
  The graph view in Airflow’s web interface, showing an overview of the tasks in
  an individual DAG and the dependencies between these tasks.
</Figure>

<Figure src="/media/airflow-tree-view.png">
  Airflow’s tree view, showing the results of multiple runs of the umbrella
  sales model DAG (most recent + historical runs). The columns show the status
  of one execution of the DAG and the rows show the status of all executions of
  a single task. Colors (which you can see in the e-book version) indicate the
  result of the corresponding task. Users can also click on the task “squares”
  for more details about a given task instance, or to reset the state of a task
  so that it can be rerun by Airflow, if desired.
</Figure>

By default, Airflow can handle failures in tasks by retrying them a couple of
times (optionally with some wait time in between), which can help tasks recover
from any intermittent failures. If retries don’t help, Airflow will record the
task as being failed, optionally notifying you about the failure if configured
to do so. Debugging task failures is pretty straightforward, as the tree view
allows you to see which tasks failed and dig into their logs. The same view also
enables you to clear the results of individual tasks to rerun them (together
with any tasks that depend on that task), allowing you to easily rerun any tasks
after you make changes to their code.

##### Incremental loading and backfilling

This property of Airflow’s schedule intervals is invaluable for implementing
efficient data pipelines, as it allows you to build incremental data pipelines.
In these incremental pipelines, each DAG run processes only data for the
corresponding time slot (the data’s _delta_) instead of having to reprocess the
entire data set every time.

Schedule intervals become even more powerful when combined with the concept of
_backfilling_, which allows you to execute a new DAG for historical schedule
intervals that occurred in the past. This feature allows you to easily create
(or _backfill_) new data sets with historical data simply by running your DAG
for these past schedule intervals. Moreover, by clearing the results of past
runs, you can also use this Airflow feature to easily rerun any historical tasks
if you make changes to your task code, allowing you to easily reprocess an
entire data set when needed.

#### When to use Airflow

##### Reasons to choose Airflow

Several key features that make Airflow ideal for implementing batch-oriented
data pipelines:

- The ability to implement pipelines using Python code allows you to create
  arbitrarily complex pipelines using anything you can dream up in Python.
- The Python foundation of Airflow makes it easy to extend and add integrations
  with many different systems. In fact, the Airflow community has already
  developed a rich collection of extensions that allow Airflow to integrate with
  many different types of databases, cloud services, and so on.
- Rich scheduling semantics allow you to run your pipelines at regular intervals
  and build efficient pipelines that use incremental processing to avoid
  expensive recomputation of existing results.
- Features such as backfilling enable you to easily (re)process historical data,
  allowing you to recompute any derived data sets after making changes to your
  code.
- Airflow’s rich web interface provides an easy view for monitoring the results
  of your pipeline runs and debugging any failures that may have occurred.

An additional advantage of Airflow is that it is open source, which guarantees
that you can build your work on Airflow without getting stuck with any vendor
lock-in.

##### Reasons not to choose Airflow

Some use cases that are not a good fit for Airflow include the following:

- Handling streaming pipelines, as Airflow is primarily designed to run
  recurring or batch-oriented tasks, rather than streaming workloads.
- Implementing highly dynamic pipelines, in which tasks are added/removed
  between every pipeline run. Although Airflow can implement this kind of
  dynamic behavior, the web interface will only show tasks that are still
  defined in the most recent version of the DAG. As such, Airflow favors
  pipelines that do not change in structure every time they run.
- Teams with little or no (Python) programming experience, as implementing DAGs
  in Python can be daunting with little Python experience. In such teams, using
  a workflow manager with a graphical interface (such as Azure Data Factory) or
  a static workflow definition may make more sense.
- Similarly, Python code in DAGs can quickly become complex for larger use
  cases. As such, implementing and maintaining Airflow DAGs require proper
  engineering rigor to keep things maintainable in the long run.

Also, Airflow is primarily a workflow/pipeline management platform and does not
(currently) include more extensive features such as maintaining data lineages,
data versioning, and so on. Should you require these features, you’ll probably
need to look at combining Airflow with other specialized tools that provide
those capabilities.

### Anatomy of an Airflow DAG

#### Collecting data from numerous sources

##### Exploring the data

```shell
curl -L "https://ll.thespacedevs.com/2.0.0/launch/upcoming"
```

#### Writing your first Airflow DAG

The nice thing about Airflow is that we can split a large job, which consists of
one or more steps, into individual “tasks” that together form a DAG. Multiple
tasks can be run in parallel, and tasks can run different technologies. For
example, we could first run a Bash script and next run a Python script.

```python nu fp=download_rocket_launches.py
import json
import pathlib

import airflow.utils.dates
import requests
import requests.exceptions as requests_exceptions
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

dag = DAG(
    dag_id="download_rocket_launches",
    description="Download rocket pictures of recently launched rockets.",
    start_date=airflow.utils.dates.days_ago(14),
    schedule_interval=None,
)

download_launches = BashOperator(
    task_id="download_launches",
    bash_command="curl -o /tmp/launches.json -L 'https://ll.thespacedevs.com/2.0.0/launch/upcoming'",  # noqa: E501
    dag=dag,
)


def _get_pictures():
    # Ensure directory exists
    pathlib.Path("/tmp/images").mkdir(parents=True, exist_ok=True)

    # Download all pictures in launches.json
    with open("/tmp/launches.json") as f:
        launches = json.load(f)
        image_urls = [launch["image"] for launch in launches["results"]]
        for image_url in image_urls:
            try:
                response = requests.get(image_url)
                image_filename = image_url.split("/")[-1]
                target_file = f"/tmp/images/{image_filename}"
                with open(target_file, "wb") as f:
                    f.write(response.content)
                print(f"Downloaded {image_url} to {target_file}")
            except requests_exceptions.MissingSchema:
                print(f"{image_url} appears to be an invalid URL.")
            except requests_exceptions.ConnectionError:
                print(f"Could not connect to {image_url}.")


get_pictures = PythonOperator(
    task_id="get_pictures", python_callable=_get_pictures, dag=dag
)

notify = BashOperator(
    task_id="notify",
    bash_command='echo "There are now $(ls /tmp/images/ | wc -l) images."',
    dag=dag,
)

download_launches >> get_pictures >> notify
```

The DAG class takes two required arguments: `dag_id` and `start_date`.

- `dag_id`: The name of the DAG displayed in the Airflow user interface (UI).
- `start_date`: The datetime at which the workflow should first start running.

Also note we set `schedule_interval` to `None`. This means the DAG will not run
automatically. For now, you can trigger it manually from the Airflow UI.

Each operator performs a single unit of work, and multiple operators together
form a workflow or DAG in Airflow.

The `BashOperator` arguments:

- `task_id`: The name of the task.
- `bash_command`: The Bash command to execute.
- `dag`: Reference to the DAG variable.

In Airflow, we can use the _binary right shift operator_ (i.e., “_rshift_”
[`>>`]) to define dependencies between tasks.

##### Tasks vs. operators

In Airflow, _operators_ have a single piece of responsibility: they exist to
perform one single piece of work. Some operators perform generic work, such as
the `BashOperator` (used to run a Bash script) or the `PythonOperator` (used to
run a Python function); others have more specific use cases, such as the
`EmailOperator` (used to send an email) or the `SimpleHTTPOperator` (used to
call an HTTP endpoint).

Tasks in Airflow manage the execution of an operator; they can be thought of as
a small wrapper or manager around an operator that ensures the operator executes
correctly. The user can focus on the work to be done by using operators, while
Airflow ensures correct execution of the work via tasks.

<Figure src="/media/airflow-tasks-vs-operators.png">
  DAGs and operators are used by Airflow users. Tasks are internal components to
  manage operator state and display state changes (e.g., started/finished) to
  the user.
</Figure>

##### Running arbitrary Python code

The `PythonOperator` in Airflow is responsible for running any Python code. Just
like the `BashOperator` used before, this and all other operators require a
`task_id`. The `python_callable` argument points to a callable, typically a
function.

#### Running a DAG in Airflow

##### Running Airflow in a Python environment

```shell
pip install apache-airflow
```

After installing Airflow, start it by initializing the metastore (a database in
which all Airflow state is stored), creating a user, copying the rocket launch
DAG into the DAGs directory, and starting the scheduler and webserver:

1. ```shell
   airflow db init
   ```
2. ```shell
   airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org
   ```
3. ```shell
   cp download_rocket_launches.py ~/airflow/dags/
   ```
4. ```shell
   airflow webserver
   ```
5. ```shell
   airflow scheduler
   ```

##### Running Airflow in Docker containers

```shell
docker run \
    -ti \
    -p 8080:8080 \
    -v /path/to/dag/download_rocket_launches.py:/opt/airflow/dags/download_rocket_launches.py \
    --entrypoint=/bin/bash \
    --name airflow \
    apache/airflow:latest \
    -c '( \
            airflow db init && \
            airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org \
        ); \
        airflow webserver & \
        airflow scheduler \
       '
```

**Note:** In a production setting, you should run the Airflow webserver,
scheduler, and metastore in separate containers.

##### Inspecting the Airflow UI

<Figure src="/media/airflow-graph-view-2.png">Airflow graph view.</Figure>

First, the DAG needs to be “on” in order to be run.

#### Running at regular intervals

In Airflow, we can schedule a DAG to run at certain intervals, for example once
an hour, day, or month. This is controlled on the DAG by setting the
`schedule_interval` argument.

```python fp=download_rocket_launches.py hl=6
...

dag = DAG(
    dag_id="download_rocket_launches",
    start_date=airflow.utils.dates.days_ago(14),
    schedule_interval="@daily",
)

...
```

When we set the `schedule_interval` to `@daily`, Airflow knew it had to run this
DAG once a day. Given the `start_date` provided to the DAG of 14 days ago, that
means the time from 14 days ago up to now can be divided into 14 equal intervals
of one day. Since both the start and end date of these 14 intervals lie in the
past, they will start running once we provide a `schedule_interval` to Airflow.

#### Handling failing tasks

**Note:** It is unnecessary to restart the entire workflow. A nice feature of
Airflow is that you can restart from the point of failure and onward, without
having to restart any previously succeeded tasks.

Click the failed task, and then click the <Scr>Clear</Scr> button in the pop-up.
It will show you the tasks you’re about to clear, meaning you will reset the
state of these tasks and Airflow will rerun them.

Click <Scr>OK!</Scr> and the failed task and its successive tasks will be
cleared.

Assuming the connectivity issues are resolved, the tasks will now run
successfully and make the whole tree view green.

After clearing the failed tasks, Airflow will automatically rerun these tasks.

### Scheduling in Airflow

#### An example: Processing user events

```python nu hl=18..20,27..30,42
from datetime import datetime
from pathlib import Path

import pandas as pd
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

dag = DAG(
    dag_id="01_unscheduled",
    start_date=datetime(2019, 1, 1),
    schedule_interval=None
)

fetch_events = BashOperator(
    task_id="fetch_events",
    bash_command=(
        "mkdir -p /data && "
        "curl -o /data/events.json "
        "http://events_api:5000/events"
    ),
    dag=dag,
)

def _calculate_stats(input_path, output_path):
    """Calculates event statistics."""
    events = pd.read_json(input_path)
    stats = events.groupby(["date", "user"]).size().reset_index()
    Path(output_path).parent.mkdir(exist_ok=True)
    stats.to_csv(output_path, index=False)

calculate_stats = PythonOperator(
    task_id="calculate_stats",
    python_callable=_calculate_stats,
    op_kwargs={
      "input_path": "/data/events.json",
      "output_path": "/data/stats.csv"
    },
    dag=dag,
)

fetch_events >> calculate_stats
```

#### Running at regular intervals

By default, the value of the `schedule_interval` argument is `None`, which means
the DAG will not be scheduled and will be run only when triggered manually from
the UI or the API.

##### Defining scheduling intervals

Airflow provides the convenient macro `@daily` for defining a daily scheduled
interval, which runs our DAG once every day at midnight.

Airflow also needs to know when we want to start executing the DAG, specified by
its start date. Based on this start date, Airflow will schedule the first
execution of our DAG to run at the first schedule interval _after_ the start
date (start + interval). Subsequent runs will continue executing at schedule
intervals following this first interval.

**Note:** Pay attention to the fact that Airflow starts tasks in an interval at
the end of the interval. If developing a DAG on January 1, 2019 at 13:00, with a
`start_date` of 01-01-2019 and `@daily` interval, this means it first starts
running at midnight. At first, nothing will happen if you run the DAG on January
1 at 13:00 until midnight is reached.

If we already know that our project has a fixed duration, we can tell Airflow to
stop running our DAG after a certain date using the `end_date` parameter.

##### Cron-based intervals

To support more complicated scheduling intervals, Airflow allows us to define
scheduling intervals using the same syntax as used by cron, a time-based job
scheduler used by Unix-like computer operating systems such as macOS and Linux.
This syntax consists of five components and is defined as follows:

```text
# ┌─────── minute (0 - 59)
# │ ┌────── hour (0 - 23)
# │ │ ┌───── day of the month (1 - 31)
# │ │ │ ┌───── month (1 - 12)
# │ │ │ │ ┌──── day of the week (0 - 6) (Sunday to Saturday;
# │ │ │ │ │ 7 is also Sunday on some systems)
# * * * * *
```

Asterisks (`*`) can be used instead of numbers to define unrestricted fields,
meaning we don’t care about the value of that field.

Examples:

- `0 * * * *` = hourly (running on the hour)
- `0 0 * * *` = daily (running at midnight)
- `0 0 * * 0` = weekly (running at midnight on Sunday)
- `0 0 1 * *` = midnight on the first of every month
- `45 23 * * SAT` = 23:45 every Saturday

Additionally, cron expressions allow you to define collections of values using a
comma (`,`) to define a list of values or a dash (`-`) to define a range of
values.

Examples:

- `0 0 * * MON,WED,FRI` = run every Monday, Wednesday, Friday at midnight
- `0 0 * * MON-FRI` = run every weekday at midnight
- `0 0,12 * * *` = run every day at 00:00 and 12:00

Airflow also provides support for several macros that represent shorthand for
commonly used scheduling intervals:

- `@once`: Schedule once and only once.
- `@hourly`: Run once an hour at the beginning of the hour.
- `@daily`: Run once a day at midnight.
- `@weekly`: Run once a week at midnight on Sunday morning.
- `@monthly`: Run once a month at midnight on the first day of the month.
- `@yearly`: Run once a year at midnight on January 1.

##### Frequency-based intervals

An important limitation of cron expressions is that they are unable to represent
certain frequency-based schedules. For example, how would you define a cron
expression that runs a DAG once every three days?

This limitation of cron stems from the nature of cron expressions, as they
define a pattern that is continuously matched against the current time to
determine whether a job should be executed. This has the advantage of making the
expressions stateless, meaning that you don’t have to remember when a previous
job was run to calculate the next interval. However, as you can see, this comes
at the price of some expressiveness. To use such a frequency-based schedule, you
can pass a `timedelta` instance (from the `datetime` module in the standard
library) as a schedule interval.

```python hl=3
dag = DAG(
    dag_id="04_time_delta",
    schedule_interval=dt.timedelta(days=3),
    start_date=dt.datetime(year=2019, month=1, day=1),
    end_date=dt.datetime(year=2019, month=1, day=5),
)
```

Of course, you can also use this approach to run your DAG every 10 minutes
(using `timedelta(minutes=10)`) or every two hours (using `timedelta(hours=2)`).

#### Processing data incrementally

Although we now have our DAG running at a daily interval (assuming we stuck with
the `@daily` schedule), we haven’t quite achieved our goal. For one, our DAG is
downloading and calculating statistics for the entire catalog of user events
every day, which is hardly efficient. Moreover, this process is only downloading
events for the past 30 days, which means we are not building any history for
earlier dates.

##### Fetching events incrementally

```shell
curl -O http://localhost:5000/events?start_date=2019-01-01&end_date=2019-01-02
```

Note that in this example `start_date` is inclusive, while `end_date` is
exclusive, meaning we are effectively fetching events that occur between
2019-01-01 00:00:00 and 2019-01-01 23:59:59.

##### Dynamic time references using execution dates

Airflow provides tasks with extra parameters. The most important of these
parameters is called the `execution_date`, which represents the date and time
for which our DAG is being executed. The end time of the schedule interval is
indicated by another parameter called the `next_execution_date`. Airflow also
provides a `prev_execution_date` parameter, which describes the start of the
previous schedule interval.

<Figure src="/media/airflow-execution-dates.png">
  Execution dates in Airflow.
</Figure>

In Airflow, we can use these execution dates by referencing them in our
operators. For example, in the `BashOperator`, we can use Airflow’s Jinja-based
templating functionality to include the execution dates dynamically in our Bash
command.

```python nu hl=7..8
fetch_events = BashOperator(
    task_id="fetch_events",
    bash_command=(
        "mkdir -p /data && "
        "curl -o /data/events.json "
        "http://events_api:5000/events?"
        "start_date={{execution_date.strftime('%Y-%m-%d')}}&"
        "end_date={{next_execution_date.strftime('%Y-%m-%d')}}"
    ),
    dag=dag,
)
```

Airflow also provides several shorthand parameters for common date formats. For
example, the `ds` and `ds_nodash` parameters are different representations of
the `execution_date`, formatted as `YYYY-MM-DD` and `YYYYMMDD`, respectively.
Similarly, `next_ds`, `next_ds_nodash`, `prev_ds`, and `prev_ds_nodash` provide
shorthand notations for the next and previous execution dates, respectively.

```python hl=7..8
fetch_events = BashOperator(
    task_id="fetch_events",
    bash_command=(
        "mkdir -p /data && "
        "curl -o /data/events.json "
        "http://events_api:5000/events?"
        "start_date={{ds}}&"
        "end_date={{next_ds}}"
    ),
    dag=dag,
)
```

##### Partitioning your data

```python hl=5
fetch_events = BashOperator(
    task_id="fetch_events",
    bash_command=(
        "mkdir -p /data/events && "
        "curl -o /data/events/{{ds}}.json "
        "http://events_api:5000/events?"
        "start_date={{ds}}&"
        "end_date={{next_ds}}"
    ),
    dag=dag,
)
```

This practice of dividing a data set into smaller, more manageable pieces is a
common strategy in data storage and processing systems and is commonly referred
to as _partitioning_, with the smaller pieces of a data set the _partitions_.

```python nu hl=1,3..4,17..18
def _calculate_stats(**context):
    """Calculates event statistics."""
    input_path = context["templates_dict"]["input_path"]
    output_path = context["templates_dict"]["output_path"]

    events = pd.read_json(input_path)
    stats = events.groupby(["date", "user"]).size().reset_index()

    Path(output_path).parent.mkdir(exist_ok=True)
    stats.to_csv(output_path, index=False)


calculate_stats = PythonOperator(
    task_id="calculate_stats",
    python_callable=_calculate_stats,
    templates_dict={
        "input_path": "/data/events/{{ds}}.json",
        "output_path": "/data/stats/{{ds}}.csv",
    },
    dag=dag,
)
```

To achieve this templating in the `PythonOperator`, we need to pass any
arguments that should be templated using the operator’s `templates_dict`
parameter. We then can retrieve the templated values inside our function from
the context object that is passed to our `_calculate_stats` function by Airflow.

#### Understanding Airflow's execution dates

##### Executing work in fixed-length intervals

Airflow runs a DAG with three parameters: a start date, a schedule interval, and
an (optional) end date. Airflow uses these three parameters to divide time into
a series of schedule intervals, starting from the given start date and
optionally ending at the end date

<Figure src="/media/airflow-scheduling-intervals.png">
  Time represented in terms of Airflow’s scheduling intervals. Assumes a daily
  interval with a start date of 2019-01-01.
</Figure>

A DAG is executed for a given interval as soon as the time slot of that interval
has passed.

An advantage of using this interval-based approach is that it is ideal for
performing the type of incremental data processing we saw in the previous
sections, as we know exactly for which interval of time a task is executing
for—the start and end of the corresponding interval. This is in stark contrast
to, for example, a time point–based scheduling system such as cron, where we
only know the current time for which our task is being executed.

<Figure src="/media/interval-based-vs-time-point-based-scheduling.png">
  Incremental processing in interval-based scheduling windows (e.g., Airflow)
  versus windows derived from time point–based systems (e.g., cron). For
  incremental (data) processing, time is typically divided into discrete time
  intervals that are processed as soon as the corresponding interval has passed.
  Interval-based scheduling approaches (such as Airflow) explicitly schedule
  tasks to run for each interval while providing exact information to each task
  concerning the start and the end of the interval. In contrast, time
  point–based scheduling approaches only execute tasks at a given time, leaving
  it up to the task itself to determine for which incremental interval the task
  is executing.
</Figure>

Airflow defines the execution date of a DAG as the start of the corresponding
interval. Conceptually, this makes sense if we consider that the execution date
marks our schedule interval rather than the moment our DAG is actually executed.

When executing a task, the start and end of the corresponding interval are
defined by the `execution_date` (the start of the interval) and the
`next_execution` date (the start of the next interval) parameters. Similarly,
the previous schedule interval can be derived using the `prev_execution_date`
and `execution_date` parameters.

However, one caveat to keep in mind when using the `prev_execution_date` and
`next_execution_date` parameters in your tasks is that these are only defined
for DAG runs following the schedule interval. As such, the values of these
parameters will be undefined for any runs that are triggered manually using
Airflow UI or CLI because Airflow cannot provide information about next or
previous schedule intervals if you are not following a schedule interval.

#### Using backfilling to fill in past gaps

##### Executing work back in time

By default, Airflow will schedule and run any past schedule intervals that have
not been run. As such, specifying a past start date and activating the
corresponding DAG will result in all intervals that have passed before the
current time being executed. This behavior is controlled by the DAG `catchup`
parameter and can be disabled by setting `catchup` to false.

```python hl=6
dag = DAG(
    dag_id="09_no_catchup",
    schedule_interval="@daily",
    start_date=dt.datetime(year=2019, month=1, day=1),
    end_date=dt.datetime(year=2019, month=1, day=5),
    catchup=False,
)
```

With this setting, the DAG will only be run for the most recent schedule
interval rather than executing all open past intervals. The default value for
catchup can be controlled from the Airflow configuration file by setting a value
for the `catchup_by_default` configuration setting.

<Figure src="/media/airflow-catchup-parameter.png">

Backfilling in Airflow. By default, Airflow will run tasks for all past
intervals up to the current time. This behavior can be disabled by setting the
`catchup` parameter of a DAG to false, in which case Airflow will only start
executing tasks from the current interval.

</Figure>

Although backfilling is a powerful concept, it is limited by the availability of
data in source systems. For example, in our example use case we can load past
events from our API by specifying a start date up to 30 days in the past.
However, as the API only provides up to 30 days of history, we cannot use
backfilling to load data from earlier days.

Backfilling can also be used to reprocess data after we have made changes in our
code. For example, say we make a change to our `calc_statistics` function to add
a new statistic. Using backfilling, we can clear past runs of our
`calc_statistics` task to reanalyze our historical data using the new code. Note
that in this case we aren’t limited by the 30-day limit of our data source, as
we have already loaded these earlier data partitions as part of our past runs.

#### Best practices for designing tasks

##### Atomicity

The term _atomicity_ is frequently used in database systems, where an atomic
transaction is considered an indivisible and irreducible series of database
operations such that either all occur or nothing occurs. Similarly, in Airflow,
tasks should be defined so that they either succeed and produce some proper
result or fail in a manner that does not affect the state of the system.

##### Idempotency

Tasks are said to be idempotent if calling the same task multiple times with the
same inputs has no additional effect.

```python
fetch_events = BashOperator(
    task_id="fetch_events",
    bash_command=(
        "mkdir -p /data/events && "
        "curl -o /data/events/{{ds}}.json "
        "http://events_api:5000/events?"
        "start_date={{ds}}&"
        "end_date={{next_ds}}"
    ),
    dag=dag,
)
```

Rerunning this task for a given date would result in the task fetching the same
set of events as its previous execution (assuming the date is within our 30-day
window), and overwriting the existing JSON file in the `/data/events` folder,
producing the same result. As such, this implementation of the fetch events task
is clearly idempotent.

### Templating tasks using the Airflow context

#### Inspecting data for processing with Airflow

##### Determining how to load incremental data

<Figure src="/media/downloading-and-inspecting-pageviews.png">
  Downloading and inspecting Wikimedia pageviews data.
</Figure>

We see the URLs follow a fixed pattern, which we can use when downloading the
data in batch fashion.

<Figure src="/media/simple-analysis-pageviews.png">
  First simple analysis on Wikimedia pageviews data.
</Figure>

#### Task context and Jinja templating

##### Templating operator arguments

```python nu
import airflow.utils.dates
from airflow import DAG
from airflow.operators.bash import BashOperator

dag = DAG(
    dag_id="chapter4_stocksense_bashoperator",
    start_date=airflow.utils.dates.days_ago(3),
    schedule_interval="@hourly",
)

get_data = BashOperator(
    task_id="get_data",
    bash_command=(
        "curl -o /tmp/wikipageviews.gz "
        "https://dumps.wikimedia.org/other/pageviews/"
        "{{ execution_date.year }}/"
        "{{ execution_date.year }}-"
        "{{ '{:02}'.format(execution_date.month) }}/"
        "pageviews-{{ execution_date.year }}"
        "{{ '{:02}'.format(execution_date.month) }}"
        "{{ '{:02}'.format(execution_date.day) }}-"
        "{{ '{:02}'.format(execution_date.hour) }}0000.gz"
    ),
    dag=dag,
)
```

Jinja is a templating engine, which replaces variables and/or expressions in a
templated string at runtime. Templating is used when you, as a programmer, don’t
know the value of something at the time of writing, but do know the value of
something at runtime.

In Airflow, you have a number of variables available at runtime from the task
context. One of these variables is `execution_date`. Airflow uses the Pendulum
library for datetimes, and `execution_date` is such a Pendulum datetime object.
It is a drop-in replacement for native Python datetime, so all methods that can
be applied to Python can also be applied to Pendulum.

##### What is available for templating?

```python nu
import airflow.utils.dates
from airflow import DAG
from airflow.operators.python import PythonOperator

dag = DAG(
    dag_id="chapter4_print_context",
    start_date=airflow.utils.dates.days_ago(3),
    schedule_interval="@daily",
)

def _print_context(**kwargs):
    print(kwargs)

print_context = PythonOperator(
    task_id="print_context",
    python_callable=_print_context,
    dag=dag,
)
```

All variables are captured in `**kwargs` and passed to the `print()` function.
All these variables are available at runtime.

All task context variables:

- `conf`: Provides access to Airflow configuration.
- `dag`: The current DAG object.
- `dag_run`: The current `DagRun` object.
- `ds`: `execution_date` formatted as `%Y-%m-%d`.
- `ds_nodash`: `execution_date` formatted as `%Y%m%d`.
- `execution_date`: The start datetime of the task's interval.
- `inlets`: Shorthand for `task.inlets`, a feature to track input data sources
  for data lineage.
- `macros`: `airflow.macros` module.
- `next_ds`: `execution_date` of the next interval (= end of current interval)
  formatted as `%Y-%m-%d`.
- `next_ds_nodash`: `execution_date` of the next interval (= end of current
  interval) formatted as `%Y%m%d`.
- `next_execution_date`: The start datetime of the task’s next interval (= end
  of current interval).
- `outlets`: Shorthand for `task.outlets`, a feature to track output data
  sources for data lineage.
- `params`: User-provided variables to the task context.
- `prev_ds`: `execution_date` of the previous interval formatted as `%Y-%m-%d`.
- `prev_ds_nodash`: `execution_date` of the previous interval formatted as
  `%Y%m%d`.
- `prev_execution_date`: The start datetime of the task's previous interval.
- `prev_execution_date_success`: Start datetime of the last successfully
  completed run of the same task (only in past).
- `prev_start_date_success`: Date and time on which the last successful run of
  the same task (only in past) was started.
- `run_id`: The `DagRun`’s `run_id` (a key typically composed of a prefix +
  datetime).
- `task`: The current operator.
- `task_instance`: The current `TaskInstance` object.
- `task_instance_key_str`: A unique identifier for the current `TaskInstance`
  (`{dag_id}__{task_id}__{ds_nodash}`).
- `templates_dict`: User-provided variables to the task context.
- `test_mode`: Whether Airflow is running in test mode (configuration property).
- `ti`: The current `TaskInstance` object, same as `task_instance`.
- `tomorrow_ds`: `ds` plus one day
- `tomorrow_ds_nodash`: `ds_nodash` plus one day
- `ts`: `execution_date` formatted according to ISO8601 format.
- `ts_nodash`: `execution_date` formatted as `%Y%m%dT%H%M%S`
- `ts_nodash_with_tz`: `ts_nodash` with time zone information.
- `var`: Helpers objects for dealing with Airflow variables.
- `yesterday_ds`: `ds` minus one day.
- `yesterday_ds_nodash`: `ds_nodash` minus one day.

##### Templating the PythonOperator

With the `BashOperator` (and all other operators in Airflow), you provide a
string to the `bash_command` argument (or whatever the argument is named in
other operators), which is automatically templated at runtime. The
`PythonOperator` is an exception to this standard, because it doesn’t take
arguments that can be templated with the runtime context, but instead a
`python_callable` argument in which the runtime context can be applied.

Since it is a function, and not a string as with all other operators, the code
within the function cannot be automatically templated.

**Note:** In Python, any object implementing `__call__()` is considered a
callable (e.g., functions/methods).

```python nu
from urllib import request

import airflow
from airflow import DAG
from airflow.operators.python import PythonOperator

dag = DAG(
    dag_id="stocksense",
    start_date=airflow.utils.dates.days_ago(1),
    schedule_interval="@hourly",
)

def _get_data(execution_date):
    year, month, day, hour, *_ = execution_date.timetuple()
    url = (
        "https://dumps.wikimedia.org/other/pageviews/"
        f"{year}/{year}-{month:0>2}/"
        f”pageviews-{year}{month:0>2}{day:0>2}-{hour:0>2}0000.gz"
    )
    output_path = "/tmp/wikipageviews.gz"
    request.urlretrieve(url, output_path)

get_data = PythonOperator(
    task_id="get_data",
    python_callable=_get_data,
    dag=dag,
)
```

In Airflow 2, the `PythonOperator` determines which context variables must be
passed along to your callable by inferring these from the callable argument
names.

```python hl=1..3
def _print_context(**context):
    start = context["execution_date"]
    end = context["next_execution_date"]
    print(f"Start: {start}, end: {end}")

print_context = PythonOperator(
    task_id="print_context",
    python_callable=_print_context,
    dag=dag,
)
```

```python hl=1
def _get_data(execution_date, **context):
  year, month, day, hour, *_ = execution_date.timetuple()
  # ...
```

The end result with this example is that a keyword with the name
`execution_date` is passed along to the `execution_date` argument and all other
variables are passed along to `**context` since they are not explicitly expected
in the function signature. Your code will be more self-explanatory and tools
such as linters and type hinting will benefit by the explicit argument
definition.

##### Providing variables to the PythonOperator

```python
def _get_data(output_path, **context):
    year, month, day, hour, *_ = context["execution_date"].timetuple()
    url = (
        "https://dumps.wikimedia.org/other/pageviews/"
        f"{year}/{year}-{month:0>2}/pageviews-{year}{month:0>2}{day:0>2}-{hour:0>2}0000.gz"
    )
    request.urlretrieve(url, output_path)
```

```python hl=4
get_data = PythonOperator(
    task_id="get_data",
    python_callable=_get_data,
    op_args=["/tmp/wikipageviews.gz"],
    dag=dag,
)
```

```python hl=4
get_data = PythonOperator(
    task_id="get_data",
    python_callable=_get_data,
    op_kwargs={"output_path": "/tmp/wikipageviews.gz"}
    dag=dag,
)
```

Providing templated strings as input for the callable function:

```python
def _get_data(year, month, day, hour, output_path, **_):
    url = (
        "https://dumps.wikimedia.org/other/pageviews/"
        f"{year}/{year}-{month:0>2}/”
        f"pageviews-{year}{month:0>2}{day:0>2}-{hour:0>2}0000.gz"
    )
    request.urlretrieve(url, output_path)

get_data = PythonOperator(
    task_id="get_data",
    python_callable=_get_data,
    op_kwargs={
        "year": "{{ execution_date.year }}",
        "month": "{{ execution_date.month }}",
        "day": "{{ execution_date.day }}",
        "hour": "{{ execution_date.hour }}",
        "output_path": "/tmp/wikipageviews.gz",
    },
    dag=dag,
)
```

##### Inspecting templated arguments

You can inspect the templated argument values after running a task by selecting
it in either the graph or tree view and clicking the <Scr>Rendered</Scr> button.

This view is visible per task instance. Consequently, a task must be scheduled
by Airflow before being able to inspect the rendered attributes. The Airflow
Command Line Interface (CLI) allows us to render templated values for any given
datetime.

```shell
airflow tasks render stocksense get_data 2019-07-19T00:00:00
```

```shell
airflow tasks render [dag id] [task id] [desired execution date]
```

#### Hooking up other systems

```python
extract_gz = BashOperator(
    task_id="extract_gz",
    bash_command="gunzip --force /tmp/wikipageviews.gz",
    dag=dag,
)

def _fetch_pageviews(pagenames):
    result = dict.fromkeys(pagenames, 0)
    with open(f"/tmp/wikipageviews", "r") as f:
        for line in f:
            domain_code, page_title, view_counts, _ = line.split(" ")
            if domain_code == "en" and page_title in pagenames:
                result[page_title] = view_counts
    print(result)

fetch_pageviews = PythonOperator(
  task_id="fetch_pageviews",
  python_callable=_fetch_pageviews,
  op_kwargs={
    "pagenames:": {
      "Google",
      "Amazon",
      "Apple",
      "Microsoft",
      "Facebook",
    }
  },
  dag=dag,
)
```

```sql nu
CREATE TABLE pageview_counts (
    pagename VARCHAR(50) NOT NULL,
    pageviewcount INT NOT NULL,
    datetime TIMESTAMP NOT NULL
);
```

An example `INSERT` query would look as follows:

```sql
INSERT INTO pageview_counts VALUES ('Google', 333, '2019-07-17T00:00:00');
```

The `PythonOperator` currently prints the results but does not write to the
database, so we’ll need a second task to write the results. In Airflow, there
are two ways of passing data between tasks:

- By using the Airflow metastore to write and read results between tasks. This
  is called XCom.
- By writing results to and from a persistent location (e.g., disk or database)
  between tasks.

Airflow tasks run independently of each other, possibly on different physical
machines depending on your setup, and therefore cannot share objects in memory.
Data between tasks must therefore be persisted elsewhere, where it resides after
a task finishes and can be read by another task.

Airflow provides one mechanism out of the box called XCom, which allows storing
and later reading any _picklable_ object in the Airflow metastore. Pickle is
Python’s serialization protocol, and serialization means converting an object in
memory to a format that can be stored on disk to be read again later, possibly
by another process. Using XComs for storing pickled objects is only suitable for
smaller objects. Since Airflow’s metastore (typically a MySQL or Postgres
database) is finite in size and pickled objects are stored in blobs in the
metastore, it’s typically advised to apply XComs only for transferring small
pieces of data such as a handful of strings (e.g., a list of names).

The alternative for transferring data between tasks is to keep the data outside
Airflow. The number of ways to store data are limitless, but typically a file on
disk is created. In the use case, we’ve fetched a few strings and integers,
which in itself are not space-consuming. With the idea in mind that more pages
might be added, and thus data size might grow in the future, we’ll think ahead
and persist the results on disk instead of using XComs.

In order to decide how to store the intermediate data, we must know where and
how the data will be used again. Since the target database is a Postgres, we’ll
use the `PostgresOperator` to insert data. First, we must install an additional
package to import the `PostgresOperator` class in our project:

```shell
pip install apache-airflow-providers-postgres
```

**Airflow 2 providers packages:** All additional pip packages are named
`apache airflow-provider-*`. Only a few core operators remain in Airflow, such
as the `BashOperator` and `PythonOperator`.

Writing `INSERT` statements to feed to the `PostgresOperator`:

```python nu
extract_gz = BashOperator(
    task_id="extract_gz",
    bash_command="gunzip --force /tmp/wikipageviews.gz",
    dag=dag,
)

def _fetch_pageviews(pagenames):
  result = dict.fromkeys(pagenames, 0)
  with open(f"/tmp/wikipageviews", "r") as f:
    for line in f:
      domain_code, page_title, view_counts, _ = line.split(" ")
      if domain_code == "en" and page_title in pagenames:
        result[page_title] = view_counts

  with open("/tmp/postgres_query.sql", "w") as f:
    for pagename, pageviewcount in result.items():
      f.write(
        "INSERT INTO pageview_counts VALUES ("
        f"'{pagename}', {pageviewcount}, '{execution_date}'"
        ");\n"
      )

fetch_pageviews = PythonOperator(
  task_id="fetch_pageviews",
  python_callable=_fetch_pageviews,
  op_kwargs={
    "pagenames:": {
      "Google",
      "Amazon",
      "Apple",
      "Microsoft",
      "Facebook",
    }
  },
  dag=dag,
)
```

```python
from airflow.providers.postgres.operators.postgres import PostgresOperator

dag = DAG(..., template_searchpath="/tmp")

write_to_postgres = PostgresOperator(
    task_id="write_to_postgres",
    postgres_conn_id="my_postgres",
    sql="postgres_query.sql",
    dag=dag,
)
```

Storing credentials in Airflow with the CLI:

```shell
airflow connections add \
    --conn-type postgres \
    --conn-host localhost \
    --conn-login postgres \
    --conn-password mysecretpassword \
    my_postgres
```

The connection is then visible in the UI (it can also be created from there). Go
to

<Scr>Admin > Connections</Scr> to view all connections stored in Airflow.

Besides a string, the content of files can also be templated. Each operator can
read and template files with specific extensions by providing the file path to
the operator. In the case of the `PostgresOperator`, the argument SQL can be
templated and thus a path to a file holding a SQL query can also be provided.
Any filepath ending in `.sql` will be read, templates in the file will be
rendered, and the queries in the file will be executed by the
`PostgresOperator`. The `template_ext` attribute holds the file extensions that
can be templated by the operator.

**Note:** Jinja requires you to provide the path to search for files that can be
templated. By default, only the path of the DAG file is searched for, but since
we’ve stored it in `/tmp`, Jinja won’t find it. To add paths for Jinja to
search, set the argument `template_searchpath` on the DAG and Jinja will
traverse the default path plus additional provided paths to search for.

Upon execution of the `PostgresOperator`, a number of things happen (figure
4.15). The `PostgresOperator` will instantiate a so-called _hook_ to communicate
with Postgres. The hook deals with creating a connection, sending queries to
Postgres and closing the connection afterward. The operator is merely passing
through the request from the user to the hook in this situation.

**Note:** An operator determines what has to be done; a hook determines how to
do something.

<Figure src="/media/postgres-hook.png">

Running a SQL script against a Postgres database involves several components.
Provide the correct settings to the `PostgresOperator`, and the `PostgresHook`
will do the work under the hood.

</Figure>

When building pipelines like these, you will only deal with operators and have
no notion of any hooks, because hooks are used internally in operators.

We can now ask questions such as “At which hour is each page most popular?”

```sql nu
SELECT
    x.pagename,
    x.hr AS "hour",
    x.average AS "average_pageviews"

FROM (
    SELECT
        pagename,
        date_part('hour', datetime) AS hr,
        AVG(pageviewcount) AS average,
        ROW_NUMBER() OVER (
            PARTITION BY pagename
            ORDER BY AVG(pageviewcount) DESC
        )
    FROM pageview_counts
    GROUP BY 1, 2
) AS x

WHERE row_number = 1;
```

### Defining dependencies between tasks

#### Basic dependencies

Adding a fan-out (one-to-multiple) dependency:

```python
from airflow.operators.dummy import DummyOperator

start = DummyOperator(task_id="start")
start >> [fetch_weather, fetch_sales]
```

Adding a fan-in (multiple-to-one) dependency:

```python
[clean_weather, clean_sales] >> join_datasets
```

#### Branching

##### Branching with tasks

Situation:

```python
def _fetch_sales(**context):
    if context["execution_date"] < ERP_CHANGE_DATE:
        _fetch_sales_old(**context)
    else:
        _fetch_sales_new(**context)

def _clean_sales(**context):
    if context["execution_date"] < ERP_CHANGE_DATE:
        _clean_sales_old(**context)
    else:
        _clean_sales_new(**context)
```

An advantage of this approach is that it allows us to incorporate some
flexibility in our DAGs without having to modify the structure of the DAG
itself. However, this approach works only in cases where the branches in our
code consist of similar tasks.

But what if loading data from the new data source requires a very different
chain of tasks? In that case, we may be better off splitting our data ingestion
into two separate sets of tasks.

Another drawback of this approach is that it is difficult to see which code
branch is being used by Airflow during a specific DAG run.

Finally, we can only encode this type of flexibility into our tasks by falling
back to general Airflow operators such as the `PythonOperator`. This prevents us
from leveraging functionality provided by more specialized Airflow operators,
which allow us to perform more complicated work with minimal coding effort.

##### Branching within the DAG

<Figure src="/media/pick-erp-system.png">
  Supporting two ERP systems using branching within the DAG, implementing
  different sets of tasks for both systems. Airflow can choose between these two
  branches using a specific branching task (here, “Pick ERP system”), which
  tells Airflow which set of downstream tasks to execute.
</Figure>

```python
fetch_sales_old = PythonOperator(...)
clean_sales_old = PythonOperator(...)

fetch_sales_new = PythonOperator(...)
clean_sales_new = PythonOperator(...)

fetch_sales_old >> clean_sales_old
fetch_sales_new >> clean_sales_new
```

Airflow provides built-in support for choosing between sets of downstream tasks
using the `BranchPythonOperator`. However, in contrast to the `PythonOperator`,
callables passed to the `BranchPythonOperator` are expected to return the ID of
a downstream task as a result of their computation. The returned ID determines
which of the downstream tasks will be executed after completion of the branch
task. Note that you can also return a list of task IDs, in which case Airflow
will execute all referenced tasks.

```python
def _pick_erp_system(**context):
    if context["execution_date"] < ERP_SWITCH_DATE:
        return "fetch_sales_old"
    else:
        return "fetch_sales_new"

pick_erp_system = BranchPythonOperator(
    task_id="pick_erp_system",
    python_callable=_pick_erp_system,
)

pick_erp_system >> [fetch_sales_old, fetch_sales_new]
start_task >> pick_erp_system
```

Airflow requires all tasks upstream of a given task to complete successfully
before that the task itself can be executed. By connecting both of our cleaning
tasks to the `join_datasets` task, we created a situation where this can never
occur, as only one of the cleaning tasks is ever executed. As a result, the
`join_datasets` task can never be executed and is skipped by Airflow.

This behavior that defines when tasks are executed is controlled by so-called
_trigger rules_ in Airflow. Trigger rules can be defined for individual tasks
using the `trigger_rule` argument, which can be passed to any operator. By
default, trigger rules are set to `all_success`, meaning that all parents of the
corresponding task need to succeed before the task can be run.

To fix this situation, we can change the trigger rule of `join_datasets` so that
it can still trigger if one of its upstream tasks is skipped. One way to achieve
this is to change the trigger rule to `none_failed`, which specifies that a task
should run as soon as all of its parents are done with executing and none have
failed.

```python
join_datasets = PythonOperator(
    ...,
    trigger_rule="none_failed",
)
```

One drawback of this approach is that we now have three edges going into the
`join_datasets` task. This doesn’t really reflect the nature of our flow, in
which we essentially want to fetch sales/weather data (choosing between the two
ERP systems first) and then feed these two data sources into `join_datasets`.
For this reason, many people choose to make the branch condition more explicit
by adding a dummy task that joins the different branches before continuing with
the DAG.

To add such a dummy task to our DAG, we can use the built-in `DummyOperator`
provided by Airflow.

```python
from airflow.operators.dummy import DummyOperator

join_branch = DummyOperator(
    task_id="join_erp_branch",
    trigger_rule="none_failed"
)
[clean_sales_old, clean_sales_new] >> join_branch
join_branch >> join_datasets
```

<Figure src="/media/dummy-task-join-branch.png">

To make the branching structure more clear, you can add an extra join task after
the branch, which ties the lineages of the branch together before continuing
with the rest of the DAG. This extra task has the added advantage that you don’t
have to change any trigger rules for other tasks in the DAG, as you can set the
required trigger rule on the join task. (Note that this means you no longer need
to set the trigger rule for the `join_datasets` task.)

</Figure>

#### Conditional tasks

Consider what happens if a colleague makes some changes to the cleaning code and
wants to use backfilling to apply these changes to the entire data set. In this
case, backfilling the DAG would also result in deploying many old instances of
our model, which we certainly aren’t interested in.

##### Conditions within tasks

```python
def _deploy(**context):
    if context["execution_date"] == ...:
        deploy_model()

deploy = PythonOperator(
    task_id="deploy_model",
    python_callable=_deploy,
)
```

Although this implementation should have the intended effect, it has the same
drawbacks as the corresponding branching implementation: it confounds the
deployment logic with the condition, we can no longer use any other built-in
operators than the `PythonOperator`, and tracking of task results in the Airflow
UI becomes less explicit.

##### Making tasks conditional

we can make our deployment conditional by adding a task that checks if the
current execution is the most recent DAG execution and adding our deployment
task downstream of this task.

```python
def _latest_only(**context):
...

latest_only = PythonOperator(
    task_id="latest_only",
    python_callable=_latest_only,
    dag=dag,
)

latest_only >> deploy_model
```

We need to fill in our `_latest_only` function to make sure that downstream
tasks are skipped if the `execution_date` does not belong to the most recent
run. To do so, we need to check our execution date and, if required, raise an
`AirflowSkipException` from our function, which is Airflow’s way of allowing us
to indicate that the condition and all its downstream tasks should be skipped,
thus skipping the deployment.

```python
from airflow.exceptions import AirflowSkipException

def _latest_only(**context):
    left_window = context["dag"].following_schedule(context["execution_date"])
    right_window = context["dag"].following_schedule(left_window)

    now = pendulum.utcnow()
    if not left_window < now <= right_window:
        raise AirflowSkipException("Not the most recent run!")
```

##### Using built-in operators

```python
from airflow.operators.latest_only import LatestOnlyOperator

latest_only = LatestOnlyOperator(
    task_id="latest_only",
    dag=dag,
)

train_model >> latest_only >> deploy_model
```

#### More about trigger rules

##### The effect of failures

This type of behavior, where the result of upstream tasks also affects
downstream tasks, is often referred to as _propagation_, as in this case the
upstream failure is _propagated_ to the downstream tasks. The effects of skipped
tasks can also be propagated downstream by the default trigger rule, resulting
in all tasks downstream of a skipped task also being skipped.

This propagation is a direct result of the definition of the `all_success`
trigger rule, which requires all of its dependencies to have been completed
successfully. As such, if it encounters a skip or failure in a dependency, it
has no other option than to fail in the same manner, thus propagating the skip
or failure.

##### Other trigger rules

| Trigger rule            | Behavior                                                                                                     | Example use case                                                                                                                 |
| ----------------------- | ------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------- |
| `all_success` (default) | Triggers when all parent tasks have been completed successfully                                              | The default trigger rule for a normal workflow                                                                                   |
| `all_failed`            | Triggers when all parent tasks have failed (or have failed as a result of a failure in their parents)        | Trigger error handling code in situations where you expected at least one success among a group of tasks                         |
| `all_done`              | Triggers when all parents are done with their execution, regardless of their resulting state                 | Execute cleanup code that you want to execute when all tasks have finished (e.g., shutting down a machine or stopping a cluster) |
| `one_failed`            | Triggers as soon as at least one parent has failed; does not wait for other parent tasks to finish executing | Quickly trigger some error handling code, such as notifications or rollbacks                                                     |
| `one_success`           | Triggers as soon as one parent succeeds; does not wait for other parent tasks to finish executing            | Quickly trigger downstream computations/ notifications as soon as one result becomes available                                   |
| `none_failed`           | Triggers if no parents have failed but have either completed successfully or been skipped                    | Join conditional branches in Airflow DAGs                                                                                        |
| `none_skipped`          | Triggers if no parents have been skipped but have either completed successfully or failed                    | Trigger a task if all upstream tasks were executed, ignoring their result(s)                                                     |
| `dummy`                 | Triggers regardless of the state of any upstream tasks                                                       | Testing                                                                                                                          |

#### Sharing data between tasks

Airflow also allows you to share small pieces of data between tasks using XComs
("cross communication"). The idea behind XComs is that they essentially allow
you to exchange messages between tasks, enabling some level of shared state.

##### Sharing data using XComs

One way to solve this problem is to use XComs to share the model identifier
between the `train_model` and `deploy_model` tasks. In this case, the
`train_model` task is responsible for pushing the XCom value, which essentially
publishes the value and makes it available for other tasks. We can publish XCom
values explicitly within our task using the `xcom_push` method, which is
available on the task instance in the Airflow context.

```python
def _train_model(**context):
    model_id = str(uuid.uuid4())
    context["task_instance"].xcom_push(key="model_id", value=model_id)

train_model = PythonOperator(
    task_id="train_model",
    python_callable=_train_model,
)
```

This call to `xcom_push` effectively tells Airflow to register our `model_id`
value as an XCom value for the corresponding task (`train_model`) and the
corresponding DAG and execution date. After running this task, you can view
these published XCom values in the web interface in the <Scr>Admin > XComs</Scr>
section.

You can retrieve the XCom value in other tasks using the `xcom_pull` method.

```python
def _deploy_model(**context):
    model_id = context["task_instance"].xcom_pull(
        task_ids="train_model", key="model_id"
    )
    print(f"Deploying model {model_id}")

deploy_model = PythonOperator(
    task_id="deploy_model",
    python_callable=_deploy_model,
)
```

Note that `xcom_pull` also allows you to define the `dag_id` and execution date
when fetching XCom values. By default, these parameters are set to the current
DAG and execution date so that `xcom_pull` only fetches values published by the
current DAG run.

You can also reference XCom variables in templates:

```python
def _deploy_model(templates_dict, **context):
    model_id = templates_dict["model_id"]
    print(f"Deploying model {model_id}")

deploy_model = PythonOperator(
    task_id="deploy_model",
    python_callable=_deploy_model,
    templates_dict={
        "model_id": "{{ task_instance.xcom_pull(task_ids='train_model', key='model_id') }}"
    },
)
```

Finally, some operators also provide support for automatically pushing XCom
values. For example, the `BashOperator` has an option `xcom_push`, which when
set to true tells the operator to push the last line written to `stdout` by the
bash command as an XCom value. Similarly, the `PythonOperator` will publish any
value returned from the Python callable as an XCom value. This means you can
also write our example as follows.

```python
def _train_model(**context):
    model_id = str(uuid.uuid4())
    return model_id
```

This works by registering the XCom under the default key `return_value`, as we
can see by looking in the <Scr>Admin</Scr> section.

##### When (not) to use XComs

Although XComs may seem pretty useful for sharing state between tasks, their use
also has some drawbacks. For example, one is that they add a hidden dependency
between tasks, as the pulling task has an implicit dependency on the task
pushing the required value. In contrast to explicit task dependencies, this
dependency is not visible in the DAG and will not be taken into account when
scheduling the tasks. As such, you’re responsible for making sure that tasks
with XCom dependencies are executed in the correct order; Airflow won’t do this
for you. These hidden dependencies become even more complicated when sharing
XCom values between different DAGs or execution dates, which is also not a
practice we would recommend following.

XComs can also be a bit of an anti-pattern when they break the atomicity of an
operator. For example, we’ve seen people use an operator to fetch an API token
in one task and then pass the token to the next task using an XCom. In this
case, a drawback was that the token expired after a couple of hours, meaning
that any rerun of the second task failed. A better approach may have been to
combine the fetching of the token in the second task, as this way both the
refreshing of the API token and performing the associated work happen in one go
(thus keeping the task atomic).

Finally, a technical limitation of XComs is that any value stored by an XCom
needs to support being serialized. This means that some Python types, such as
lambdas or many multiprocessing related classes, generally cannot be stored in
an XCom (though you probably shouldn’t want to do that anyway). Additionally,
the size of an XCom value may be limited by the backend used to store them. By
default, XComs are stored in the Airflow metastore and are subject to the
following size limits:

- _SQLite_—Stored as BLOB type, 2GB limit
- _PostgreSQL_—Stored as BYTEA type, 1 GB limit
- _MySQL_—Stored as BLOB type, 64 KB limit

That being said, XComs can be a powerful tool when used appropriately. Just make
sure to carefully consider their usage and to clearly document the dependencies
they introduce between tasks to avoid any surprises down the road.

##### Using custom XCom backends

A limitation of using the Airflow metastore to store XComs is that it generally
does not scale well for larger data volumes. This means you’d typically want to
use XComs for storing individual values or small results but not for larger data
sets.

To make XComs more flexible, Airflow 2 introduces an option for specifying a
custom XCom backend for your Airflow deployment. This option essentially allows
you to define a custom class that Airflow will use for storing/retrieving XComs.
The only requirement is that this class inherits from the `BaseXCom` base class
and implements two static methods for serializing and deserializing values,
respectively.

```python fp=includes/custom_xcom_backend.py
from typing import Any
from airflow.models.xcom import BaseXCom

class CustomXComBackend(BaseXCom):

    @staticmethod
    def serialize_value(value: Any):
        ...

    @staticmethod
    def deserialize_value(result) -> Any:
        ...
```

In this custom backend class, the serialize method is called whenever an XCom
value is pushed within an operator, whereas the deserialize method is called
when XCom values are pulled from the backend. Once you have the desired backend
class, you can configure Airflow to use the class with the `xcom_backend`
parameter in the Airflow config.

Custom XCom backends greatly expand the options you have for storing XCom
values. For example, if you’d like to store larger XCom values in relatively
cheap and scalable cloud storage, you could implement a custom backend for cloud
services such as Azure Blob storage, Amazon’s S3, or Google’s GCS. As Airflow 2
matures, we expect backends for common services to become more generally
available, meaning you won’t have to build your own backends for these services.

#### Chaining Python tasks with the Taskflow API

##### Simplifying Python tasks with the Taskflow API

```python
def _train_model(**context):
    model_id = str(uuid.uuid4())
    context["task_instance"].xcom_push(key="model_id", value=model_id)

def _deploy_model(**context):
    model_id = context["task_instance"].xcom_pull(
        task_ids="train_model", key="model_id"
    )
    print(f"Deploying model {model_id}")

with DAG(...) as dag:
    ...

    train_model = PythonOperator(
        task_id="train_model",
        python_callable=_train_model,
    )

    deploy_model = PythonOperator(
        task_id="deploy_model",
        python_callable=_deploy_model,
    )

    ...

    join_datasets >> train_model >> deploy_model
```

The Taskflow API aims to simplify the definition of this type of
(`PythonOperator` based) task by making it easier to convert Python functions to
tasks and making the sharing of variables via XComs between these tasks more
explicit in the DAG definition.

```python
from airflow.decorators import task

with DAG(
    dag_id="12_taskflow",
    start_date=airflow.utils.dates.days_ago(3),
    schedule_interval="@daily",
) as dag:

    @task
    def train_model():
        model_id = str(uuid.uuid4())
        return model_id

    @task
    def deploy_model(model_id: str):
        print(f"Deploying model {model_id}")

    model_id = train_model()
    deploy_model(model_id)
```

##### When (not) to use the Taskflow API

```python
import uuid

import airflow

from airflow import DAG
from airflow.decorators import task
from airflow.operators.dummy import DummyOperator

with DAG(
    dag_id="13_taskflow_full",
    start_date=airflow.utils.dates.days_ago(3),
    schedule_interval="@daily",
) as dag:
    start = DummyOperator(task_id="start")

    fetch_sales = DummyOperator(task_id="fetch_sales")
    clean_sales = DummyOperator(task_id="clean_sales")

    fetch_weather = DummyOperator(task_id="fetch_weather")
    clean_weather = DummyOperator(task_id="clean_weather")

    join_datasets = DummyOperator(task_id="join_datasets")

    start >> [fetch_sales, fetch_weather]
    fetch_sales >> clean_sales
    fetch_weather >> clean_weather
    [clean_sales, clean_weather] >> join_datasets

    @task
    def train_model():
        model_id = str(uuid.uuid4())
        return model_id

    @task
    def deploy_model(model_id: str):
        print(f"Deploying model {model_id}")

    model_id = train_model()
    deploy_model(model_id)

    join_datasets >> model_id
```

Any data passed between Taskflow-style tasks will be stored as XComs. This means
that all passed values are subject to the technical limitations of XComs (i.e.,
they must be serializable). Moreover, the size of data sets passed between tasks
may be limited by the XCom backend used by your Airflow deployment.

## Beyong the basics

### Triggering workflows

#### Polling conditions with sensors

One way to solve this in Airflow is with the help of _sensors_, which are a
special type (subclass) of operators. Sensors continuously poll for certain
conditions to be true and succeed if so. If false, the sensor will wait and try
again until either the condition is true or a timeout is eventually reached.

```python
from airflow.sensors.filesystem import FileSensor

wait_for_supermarket_1 = FileSensor(
    task_id="wait_for_supermarket_1",
    filepath="/data/supermarket1/data.csv",
)
```

This `FileSensor` will check for the existence of `/data/supermarket1/data.csv`
and return true if the file exists. If not, it returns false and the sensor will
wait for a given period (default 60 seconds) and try again. Both operators
(sensors are also operators) and DAGs have configurable timeouts, and the sensor
will continue checking the condition until a timeout is reached.

The sensor _pokes_ for the availability of the given file. _Poking_
(configurable with the `poke_interval` argument) is Airflow’s name for running
the sensor and checking the sensor condition.

##### Polling custom conditions

Airflow’s `FileSensor` supports wildcards to match, for example, `data-*.csv`,
which will match any file matching the pattern. So, if, for example, the first
file `data-01.csv` is delivered while others are still being uploaded to the
shared storage by the supermarket, the `FileSensor` would return true and the
workflow would continue to the `copy_to_raw` task, which is undesirable.

Therefore, we agreed with the supermarkets to write a file named `_SUCCESS` as
the last part of uploading, to indicate the full daily data set was uploaded.
The data team decided they want to check for both the existence of one or more
files named `data-*.csv` and one file named `_SUCCESS`. Under the hood the
`FileSensor` uses _globbing_ to match patterns against file or directory names.
While globbing (similar to regex but more limited in functionality) would be
able to match multiple patterns with a complex pattern, a more readable approach
is to implement the two checks with the `PythonSensor`.

The `PythonSensor` is similar to the `PythonOperator` in the sense that you
supply a Python callable (function, method, etc.) to execute. However, the
`PythonSensor` callable is limited to returning a Boolean value: true to
indicate the condition is met successfully, false to indicate it is not.

```python
from pathlib import Path

from airflow.sensors.python import PythonSensor

def _wait_for_supermarket(supermarket_id):
    supermarket_path = Path("/data/" + supermarket_id)
    data_files = supermarket_path.glob("data-*.csv")
    success_file = supermarket_path / "_SUCCESS"
    return data_files and success_file.exists()

wait_for_supermarket_1 = PythonSensor(
    task_id="wait_for_supermarket_1",
    python_callable=_wait_for_supermarket,
    op_kwargs={"supermarket_id": "supermarket1"},
    dag=dag,
)
```

##### Sensors outside the happy flow

Sensors accept a `timeout` argument, which holds the maximum number of seconds a
sensor is allowed to run. If, at the start of the next poke, the number of
running seconds turns out to be higher than the number set to timeout, the
sensor will fail.

By default, the sensor timeout is set to seven days. If the DAG
`schedule_interval` is set to once a day, this will lead to an undesired
snowball effect—which is surprisingly easy to encounter with many DAGs! The DAG
runs once a day, and supermarkets 2, 3, and 4 will fail after seven days, as
shown in figure 6.7. However, new DAG runs are added every day and the sensors
for those respective days are started, and as a result more and more tasks start
running. Here’s the catch: there’s a limit to the number of tasks Airflow can
handle and will run (on various levels).

It is important to understand there are limits to the maximum number of running
tasks on various levels in Airflow; the number of tasks per DAG, the number of
tasks on a global Airflow level, the number of DAG runs per DAG, and so on.

The DAG class has a `concurrency` argument, which controls how many
simultaneously running tasks are allowed within that DAG.

```python hl=5
dag = DAG(
    dag_id="couponing_app",
    start_date=datetime(2019, 1, 1),
    schedule_interval="0 0 * * *",
    concurrency=50,
)
```

We ran the DAG with all defaults, which is 16 concurrent tasks per DAG. [...].
This behavior is often referred to as _sensor deadlock_. In this example, the
maximum number of running tasks in the supermarket couponing DAG is reached, and
thus the impact is limited to that DAG, while other DAGs can still run. However,
once the global Airflow limit of maximum tasks is reached, your entire system is
stalled.

The sensor class takes an argument `mode`, which can be set to either `poke` or
`reschedule`. By default, it's set to `poke`, leading to the blocking behavior.

This means the sensor task occupies a task slot as long as it’s running. Once in
a while, it pokes the condition and then does nothing, but still occupies a task
slot. The sensor `reschedule` mode releases the slot after it has finished
poking, so it _only_ occupies a slot while it’s doing actual work.

The number of concurrent tasks can also be controlled by several configuration
options on the global Airflow level.

#### Triggering other DAGs

We received a question from the analyst team about if the metrics could also be
made available directly after processing instead of having to wait for other
supermarkets to deliver their data and run it through the pipeline.

We could set the `create_metrics` task as a downstream task after every
`process_supermarket_*` task.

Suppose the `create_metrics` task evolved into multiple tasks, making the DAG
structure more complex and resulting in more repeated tasks.

One option to circumvent repeated tasks with (almost) equal functionality is to
split your DAG into multiple smaller DAGs where each takes care of part of the
total workflow. One benefit is you can call DAG 2 multiple times from DAG 1,
instead of one single DAG holding multiple (duplicated) tasks from DAG 2.
Whether this is possible or desirable depends on many things, such as the
complexity of the workflow. If, for example, you’d like to be able to create the
metrics without having to wait for the workflow to complete according to its
schedule, but instead trigger it manually whenever you’d like, then it could
make sense to split it into two separate DAGs.

This scenario can be achieved with the `TriggerDagRunOperator`. This operator
allows triggering other DAGs, which you can apply to decouple parts of a
workflow.

```python
import airflow.utils.dates
from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator

dag1 = DAG(
    dag_id="ingest_supermarket_data",
    start_date=airflow.utils.dates.days_ago(3),
    schedule_interval="0 16 * * *",
)

for supermarket_id in range(1, 5):
    # ...

    trigger_create_metrics_dag = TriggerDagRunOperator(
        task_id=f"trigger_create_metrics_dag_supermarket_{supermarket_id}",
        trigger_dag_id="create_metrics",
        dag=dag1,
    )

dag2 = DAG(
    dag_id="create_metrics",
    start_date=airflow.utils.dates.days_ago(3),
    schedule_interval=None,
)

# ...
```

Visually, in the Airflow UI there is almost no difference between a scheduled
DAG, manually triggered DAG, or an automatically triggered DAG. Two small
details in the tree view tell you whether a DAG was triggered or started by a
schedule. First, scheduled DAG runs and their task instances show a black
border. Second, each DAG run holds a field `run_id`. The value of the `run_id`
starts with one of the following:

- `scheduled__` to indicate the DAG run started because of its schedule
- `backfill__` to indicate the DAG run started by a backfill job
- `manual__` to indicate the DAG run started by a manual action (e.g., pressing
  the Trigger Dag button, or triggered by a `TriggerDagRunOperator`)

Hovering over the DAG run circle displays a tooltip showing the `run_id` value,
telling us how the DAG started running.

##### Backfilling with the TriggerDagRunOperator

What if you changed some logic in the `process_*` tasks and wanted to rerun the
DAGs from there on? In a single DAG you could clear the state of the `process_*`
and corresponding downstream tasks. However, clearing tasks only clears tasks
within the same DAG. Tasks downstream of a `TriggerDagRunOperator` in another
DAG are not cleared, so be well aware of this behavior.

Clearing tasks in a DAG, including a `TriggerDagRunOperator`, will trigger a new
DAG run instead of clearing the corresponding previously triggered DAG runs.

##### Polling the state of other DAGs

The preceding DAG works as long as there is no dependency from the to be
triggered DAGs back to the triggering DAG. In other words, the first DAG can
trigger the downstream DAG whenever, without having to check any conditions.

If the DAGs become very complex, for clarity the first DAG could be split across
multiple DAGs, and a corresponding `TriggerDagRunOperator` task could be made
for each corresponding DAG. Also, one DAG triggering multiple downstream DAGs is
a possible scenario with the `TriggerDagRunOperator`.

<Figure src="/media/various-inter-dag-dependencies.png">

Various inter-DAG dependencies possible with the `TriggerDagRunOperator`.

</Figure>

But what if multiple triggering DAGs must complete before another DAG can start
running? For example, what if DAGs 1, 2, and 3 each extract, transform, and load
a data set, and you’d like to run DAG 4 _only_ once all three DAGs have
completed, for example to compute a set of aggregated metrics? Airflow manages
dependencies between tasks within one single DAG; however, it does _not_ provide
a mechanism for inter-DAG dependencies.

For this situation we could apply the `ExternalTaskSensor`, which is a sensor
poking the state of tasks in other DAGs. This way the `wait_for_etl_dag{1,2,3}`
tasks act as a proxy to ensure the completed state of all three DAGs before
finally executing the report task.

The way the `ExternalTaskSensor` works is by pointing it to a task in another
DAG to check its state.

```python
import airflow.utils.dates
from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.sensors.external_task import ExternalTaskSensor

dag1 = DAG(dag_id="ingest_supermarket_data", schedule_interval="0 16 * * *", ...)
dag2 = DAG(schedule_interval="0 16 * * *", ...)

DummyOperator(task_id="copy_to_raw", dag=dag1) >> DummyOperator(task_id="process_supermarket", dag=dag1)

wait = ExternalTaskSensor(
    task_id="wait_for_process_supermarket",
    external_dag_id="ingest_supermarket_data",
    external_task_id="process_supermarket",
    dag=dag2,
)
report = DummyOperator(task_id="report", dag=dag2)
wait >> report
```

The default behavior is such that the `ExternalTaskSensor` simply checks for a
successful state of a task with the exact same execution date as itself. So, if
an `ExternalTaskSensor` runs with an execution date of 2019-10-12T18:00:00, it
would query the Airflow metastore for the given task, also with an execution
date of 2019-10- 12T18:00:00. Now let’s say both DAGs have a different schedule
interval; then these would not align and thus the `ExternalTaskSensor` would
never find the corresponding task!

In case the schedule intervals do not align we can offset, by which the
`ExternalTaskSensor` must search for the task in the other DAG. This offset is
controlled by the `execution_delta` argument on the `ExternalTaskSensor`. It
expects a `timedelta` object, and it’s important to know it operates
counterintuitive from what you expect. The given `timedelta` is subtracted from
the `execution_date`, meaning that a positive `timedelta` actually looks back in
time.

```python
from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.sensors.external_task import ExternalTaskSensor

dag1 = DAG(dag_id="dag1", schedule_interval="0 16 * * *")
dag2 = DAG(dag_id="dag2", schedule_interval="0 20 * * *")

DummyOperator(task_id="etl", dag=dag1)
ExternalTaskSensor(
    task_id="wait_for_etl",
    external_dag_id="dag1",
    external_task_id="etl",
    execution_delta=datetime.timedelta(hours=4),
    dag=dag2,
)
```

Note that checking a task using the `ExternalTaskSensor` where the other DAG has
a different interval period, for example, DAG 1 runs once a day and DAG 2 runs
every five hours, complicates setting a good value for `execution_delta`. For
this use case, it’s possible to provide a function returning a list of
timedeltas via the `execution_date_fn` argument.

#### Starting workflows with REST/CLI

In addition to triggering DAGs from other DAGs, they can also be triggered via
the REST API and CLI. This can be useful if you want to start workflows from
outside Airflow (e.g., as part of a CI/CD pipeline). Or, data arriving at random
times in an AWS S3 bucket can be processed by setting a Lambda function to call
the REST API, triggering a DAG, instead of having to run sensors polling all the
time.

Using the Airflow CLI, we can trigger a DAG as follows.

```shell
airflow dags trigger dag1
```

This triggers `dag1` with the execution date set to the current date and time.
The DAG `run_id` is prefixed with `manual__` to indicate it was triggered
manually, or from outside Airflow. The CLI accepts additional configuration to
the triggered DAG.

```shell
airflow dags trigger -c '{"supermarket_id": 1}' dag1
airflow dags trigger --conf '{"supermarket_id": 1}' dag1
```

This piece of configuration is then available in all tasks of the triggered DAG
run via the task context variables.

```python nu
import airflow.utils.dates
from airflow import DAG
from airflow.operators.python import PythonOperator

dag = DAG(
    dag_id="print_dag_run_conf",
    start_date=airflow.utils.dates.days_ago(3),
    schedule_interval=None,
)

def print_conf(**context):
    print(context["dag_run"].conf)

process = PythonOperator(
    task_id="process",
    python_callable=print_conf,
    dag=dag,
)
```

As a result, if you have a DAG in which you run copies of tasks simply to
support different variables, this becomes a whole lot more concise with the DAG
run conf, since it allows you to insert variables into the pipeline. However,
note that the DAG has no schedule interval (i.e., it only runs when triggered).
If the logic in your DAG relies on a DAG run conf, then it won’t be possible to
run on a schedule since that doesn’t provide any DAG run conf.

Similarly, it is also possible to use the REST API for the same result (e.g., in
case you have no access to the CLI but your Airflow instance can be reached over
HTTP).

```shell
curl \
    -u admin:admin \
    -X POST \
    "http://localhost:8080/api/v1/dags/print_dag_run_conf/dagRuns" \
    -H "Content-Type: application/json" \
    -d '{"conf": {}}'

curl \
    -u admin:admin \
    -X POST \
    "http://localhost:8080/api/v1/dags/print_dag_run_conf/dagRuns" \
    -H "Content-Type: application/json" \
    -d '{"conf": {"supermarket": 1}}'
```

This could be convenient when triggering DAG from outside Airflow, for example
from your CI/CD system.

**Note:** Sending a plaintext username/password is not desirable; consult the
Airflow API authentication documentation for other authentication methods.

**Note:** The endpoint requires a piece of data, even if no additional
configuration is given.

### Communicating with external systems

#### Connecting to cloud services

#### Moving data from between systems

### Building custom components

#### Starting with a PythonOperator

#### Building a custom hook

#### Building a custom operator

#### Building custom sensors

#### Packaging your components

### Testing

#### Getting started with testing

#### Working with DAGs and task context in tests

#### Using tests from development

#### Emulate production environments with Whirl

#### Create DTAP environments

### Running tasks in containers

#### Challenges of many different operators

#### Introducing containers

#### Containers and Airflow

#### Running tasks in Docker

#### Running tasks in Kubernetes

### Airflow in practice

#### Best practices

#### Writing clean DAGs

#### Designing reproducible tasks

#### Handling data efficiently

#### Managing your resources

### Operating Airflow in production

#### Airflow architectures

#### Installing each executor

#### Capturing logs of all Airflow processes

#### Visualizing and monitoring Airflow metrics

#### How to get notified of a failing task

### Securing Airflow

#### Securing the Airflow web interface

#### Encrypting data at rest

#### Connecting with an LDAP service

#### Encrypting traffic to the webserver

#### Fetching credentials from secret management

### Project: Finding the fastest way to get around NYC

#### Understanding the data

#### Extracting the data

#### Applying similar transformations to data

#### Structuring a data pipeline

#### Developing idempotent data pipelines

## In the clouds

### Airflow in the clouds

#### Designing (cloud) deployment strategies

#### Cloud-specific operators and hooks

#### Managed services

#### Choosing a deployment strategy

### Airflow on AWS

#### Deploying Airflow in AWS

#### AWS-specific hooks and operators

#### Use case: Serverless movie ranking with AWS Athena

### Airflow on Azure

#### Deploying Airflow in Azure

#### Azure-specific hooks/operators

#### Example: Serverless movie ranking with Azure

### Airflow in GCP

#### Deploying Airflow in GCP

#### GCP-specific hooks and operators

#### Use case: Serverless movie ranking on GCP
