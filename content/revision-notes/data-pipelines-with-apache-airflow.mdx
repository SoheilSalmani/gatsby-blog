---
title: Data Pipelines with Apache Airflow
resourceId: "9781617296901"
stoppedAt: Best practices for designing tasks
---

## Getting started

### Meet Apache Airflow

#### Introducing data pipelines

Data pipelines generally consist of several tasks or actions that need to be
executed to achieve the desired result. For example, say we want to build a
small weather dashboard that tells us what the weather will be like in the
coming week. To implement this live weather dashboard, we need to perform
something like the following steps:

1. Fetch weather forecast data from a weather API.
2. Clean or otherwise transform the fetched data (e.g., converting temperatures
   from Fahrenheit to Celsius or vice versa), so that the data suits our
   purpose.
3. Push the transformed data to the weather dashboard.

As such, we need to make sure that this implicit task order is also enforced
when running this data process.

##### Data pipelines as graphs

One way to make dependencies between tasks more explicit is to draw the data
pipeline as a _directed graph_.

<Figure src="/media/weather-data-pipeline-graph.png">
  Graph representation of the data pipeline for the weather dashboard. Nodes
  represent tasks and directed edges represent dependencies between tasks (with
  an edge pointing from task A to task B, indicating that task A needs to be run
  before task B).
</Figure>

This type of graph is typically called a _directed acyclic graph (DAG)_, as the
graph contains directed edges and does not contain any loops or cycles
(acyclic). This acyclic property is extremely important, as it prevents us from
running into circular dependencies between tasks (where task A depends on task B
and vice versa). This logical inconsistency leads to a deadlock type of
situation, in which neither task 2 nor 3 can run, preventing us from executing
the graph.

The acyclic property of DAGs is used by Airflow (and many other workflow
managers) to efficiently resolve and execute these graphs of tasks.

##### Executing a pipeline graph

A nice property of this DAG representation is that it provides a relatively
straightforward algorithm that we can use for running the pipeline.
Conceptually, this algorithm consists of the following steps:

1. For each open (= uncompleted) task in the graph, do the following: – For each
   edge pointing toward the task, check if the “upstream” task on the other end
   of the edge has been completed. – If all upstream tasks have been completed,
   add the task under consideration to a queue of tasks to be executed.
2. Execute the tasks in the execution queue, marking them completed once they
   finish performing their work.
3. Jump back to step 1 and repeat until all tasks in the graph have been
   completed.

##### Pipeline graphs vs. sequential scripts

To build a pipeline for training the ML model, we need to implement something
like the following steps:

1. Prepare the sales data by doing the following:
   - Fetching the sales data from the source system
   - Cleaning/transforming the sales data to fit requirements
2. Prepare the weather data by doing the following:
   - Fetching the weather forecast data from an API
   - Cleaning/transforming the weather data to fit requirements
3. Combine the sales and weather data sets to create the combined data set that
   can be used as input for creating a predictive ML model.
4. Train the ML model using the combined data set.
5. Deploy the ML model so that it can be used by the business.

<Figure src="/media/umbrella-company-data-pipeline.png">
  Overview of the umbrella demand use case, in which historical weather and
  sales data are used to train a model that predicts future sales demands
  depending on weather forecasts.
</Figure>

One important difference from our previous example is that the first steps of
this pipeline are in fact independent of each other, as they involve two
separate data sets. This is clearly illustrated by the two separate branches in
the graph representation of the pipeline, which can be executed in parallel if
we apply our graph execution algorithm, making better use of available resources
and potentially decreasing the running time of a pipeline compared to executing
the tasks sequentially.

Another useful property of the graph-based representation is that it clearly
separates pipelines into small incremental tasks rather than having one
monolithic script or process that does all the work. Although having a single
monolithic script may not initially seem like that much of a problem, it can
introduce some inefficiencies when tasks in the pipeline fail, as we would have
to rerun the entire script. In contrast, in the graph representation, we need
only to rerun any failing tasks (and any downstream dependencies).

##### Running pipeline using workflow managers

The challenge of running graphs of dependent tasks is hardly a new problem in
computing. Over the years, many so-called “workflow management” solutions have
been developed to tackle this problem, which generally allow you to define and
execute graphs of tasks as workflows or pipelines.

Although each of these workflow managers has its own strengths and weaknesses,
they all provide similar core functionality that allows you to define and run
pipelines containing multiple tasks with dependencies.

One of the key differences between these tools is how they define their
workflows. For example, tools such as Oozie use static (XML) files to define
workflows, which provides legible workflows but limited flexibility. Other
solutions such as Luigi and Airflow allow you to define workflows as code, which
provides greater flexibility but can be more challenging to read and test
(depending on the coding skills of the person implementing the workflow).

Other key differences lie in the extent of features provided by the workflow
manager. For example, tools such as Make and Luigi do not provide built-in
support for scheduling workflows, meaning that you’ll need an extra tool like
Cron if you want to run your workflow on a recurring schedule. Other tools may
provide extra functionality such as scheduling, monitoring, user-friendly web
interfaces, and so on built into the platform, meaning that you don’t have to
stitch together multiple tools yourself to get these features.

All in all, picking the right workflow management solution for your needs will
require some careful consideration of the key features of the different
solutions and how they fit your requirements.

#### Introducing Airflow

##### Defining pipelines flexibly in (Python) code

In Airflow, you define your DAGs using Python code in DAG files, which are
essentially Python scripts that describe the structure of the corresponding DAG.

One advantage of defining Airflow DAGs in Python code is that this programmatic
approach provides you with a lot of flexibility for building DAGs. For example,
as we will see later in this book, you can use Python code to dynamically
generate optional tasks depending on certain conditions or even generate entire
DAGs based on external metadata or configuration files. This flexibility gives a
great deal of customization in how you build your pipelines, allowing you to fit
Airflow to your needs for building arbitrarily complex pipelines.

<Figure src="/media/airflow-dag-file.png">
  Airflow pipelines are defined as DAGs using Python code in DAG files. Each DAG
  file typically defines one DAG, which describes the different tasks and their
  dependencies. Besides this, the DAG also defines a schedule interval that
  determines when the DAG is executed by Airflow.
</Figure>

##### Scheduling and executing pipelines

Airflow allows you to define a schedule interval for each DAG, which determines
exactly when your pipeline is run by Airflow. This way, you can tell Airflow to
execute your DAG every hour, every day, every week, and so on, or even use more
complicated schedule intervals based on Cron-like expressions.

At a high level, Airflow is organized into three main components:

- _The Airflow scheduler_—Parses DAGs, checks their schedule interval, and (if
  the DAGs’ schedule has passed) starts scheduling the DAGs’ tasks for execution
  by passing them to the Airflow workers.
- _The Airflow workers_—Pick up tasks that are scheduled for execution and
  execute them. As such, the workers are responsible for actually “doing the
  work.”
- _The Airflow webserver_—Visualizes the DAGs parsed by the scheduler and
  provides the main interface for users to monitor DAG runs and their results.

<Figure src="/media/airflow-scheduler-workers-webserver.png">
  Overview of the main components involved in Airflow (e.g., the Airflow
  webserver, scheduler, and workers).
</Figure>

<Figure src="/media/airflow-process-involved.png">
  Schematic overview of the process involved in developing and executing
  pipelines as DAGs using Airflow.
</Figure>

Once tasks have been queued for execution, they are picked up by a pool of
Airflow workers that execute tasks in parallel and track their results. These
results are communicated to Airflow’s metastore so that users can track the
progress of tasks and view their logs using the Airflow web interface (provided
by the Airflow webserver).

##### Monitoring and handling failures

<Figure src="/media/airflow-main-page.png">
  The main page of Airflow’s web interface, showing an overview of the available
  DAGs and their recent results.
</Figure>

<Figure src="/media/airflow-graph-view.png">
  The graph view in Airflow’s web interface, showing an overview of the tasks in
  an individual DAG and the dependencies between these tasks.
</Figure>

<Figure src="/media/airflow-tree-view.png">
  Airflow’s tree view, showing the results of multiple runs of the umbrella
  sales model DAG (most recent + historical runs). The columns show the status
  of one execution of the DAG and the rows show the status of all executions of
  a single task. Colors (which you can see in the e-book version) indicate the
  result of the corresponding task. Users can also click on the task “squares”
  for more details about a given task instance, or to reset the state of a task
  so that it can be rerun by Airflow, if desired.
</Figure>

By default, Airflow can handle failures in tasks by retrying them a couple of
times (optionally with some wait time in between), which can help tasks recover
from any intermittent failures. If retries don’t help, Airflow will record the
task as being failed, optionally notifying you about the failure if configured
to do so. Debugging task failures is pretty straightforward, as the tree view
allows you to see which tasks failed and dig into their logs. The same view also
enables you to clear the results of individual tasks to rerun them (together
with any tasks that depend on that task), allowing you to easily rerun any tasks
after you make changes to their code.

##### Incremental loading and backfilling

This property of Airflow’s schedule intervals is invaluable for implementing
efficient data pipelines, as it allows you to build incremental data pipelines.
In these incremental pipelines, each DAG run processes only data for the
corresponding time slot (the data’s _delta_) instead of having to reprocess the
entire data set every time.

Schedule intervals become even more powerful when combined with the concept of
_backfilling_, which allows you to execute a new DAG for historical schedule
intervals that occurred in the past. This feature allows you to easily create
(or _backfill_) new data sets with historical data simply by running your DAG
for these past schedule intervals. Moreover, by clearing the results of past
runs, you can also use this Airflow feature to easily rerun any historical tasks
if you make changes to your task code, allowing you to easily reprocess an
entire data set when needed.

#### When to use Airflow

##### Reasons to choose Airflow

Several key features that make Airflow ideal for implementing batch-oriented
data pipelines:

- The ability to implement pipelines using Python code allows you to create
  arbitrarily complex pipelines using anything you can dream up in Python.
- The Python foundation of Airflow makes it easy to extend and add integrations
  with many different systems. In fact, the Airflow community has already
  developed a rich collection of extensions that allow Airflow to integrate with
  many different types of databases, cloud services, and so on.
- Rich scheduling semantics allow you to run your pipelines at regular intervals
  and build efficient pipelines that use incremental processing to avoid
  expensive recomputation of existing results.
- Features such as backfilling enable you to easily (re)process historical data,
  allowing you to recompute any derived data sets after making changes to your
  code.
- Airflow’s rich web interface provides an easy view for monitoring the results
  of your pipeline runs and debugging any failures that may have occurred.

An additional advantage of Airflow is that it is open source, which guarantees
that you can build your work on Airflow without getting stuck with any vendor
lock-in.

##### Reasons not to choose Airflow

Some use cases that are not a good fit for Airflow include the following:

- Handling streaming pipelines, as Airflow is primarily designed to run
  recurring or batch-oriented tasks, rather than streaming workloads.
- Implementing highly dynamic pipelines, in which tasks are added/removed
  between every pipeline run. Although Airflow can implement this kind of
  dynamic behavior, the web interface will only show tasks that are still
  defined in the most recent version of the DAG. As such, Airflow favors
  pipelines that do not change in structure every time they run.
- Teams with little or no (Python) programming experience, as implementing DAGs
  in Python can be daunting with little Python experience. In such teams, using
  a workflow manager with a graphical interface (such as Azure Data Factory) or
  a static workflow definition may make more sense.
- Similarly, Python code in DAGs can quickly become complex for larger use
  cases. As such, implementing and maintaining Airflow DAGs require proper
  engineering rigor to keep things maintainable in the long run.

Also, Airflow is primarily a workflow/pipeline management platform and does not
(currently) include more extensive features such as maintaining data lineages,
data versioning, and so on. Should you require these features, you’ll probably
need to look at combining Airflow with other specialized tools that provide
those capabilities.

### Anatomy of an Airflow DAG

#### Collecting data from numerous sources

##### Exploring the data

```shell
curl -L "https://ll.thespacedevs.com/2.0.0/launch/upcoming"
```

#### Writing your first Airflow DAG

The nice thing about Airflow is that we can split a large job, which consists of
one or more steps, into individual “tasks” that together form a DAG. Multiple
tasks can be run in parallel, and tasks can run different technologies. For
example, we could first run a Bash script and next run a Python script.

```python nu fp=download_rocket_launches.py
import json
import pathlib

import airflow.utils.dates
import requests
import requests.exceptions as requests_exceptions
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

dag = DAG(
    dag_id="download_rocket_launches",
    description="Download rocket pictures of recently launched rockets.",
    start_date=airflow.utils.dates.days_ago(14),
    schedule_interval=None,
)

download_launches = BashOperator(
    task_id="download_launches",
    bash_command="curl -o /tmp/launches.json -L 'https://ll.thespacedevs.com/2.0.0/launch/upcoming'",  # noqa: E501
    dag=dag,
)


def _get_pictures():
    # Ensure directory exists
    pathlib.Path("/tmp/images").mkdir(parents=True, exist_ok=True)

    # Download all pictures in launches.json
    with open("/tmp/launches.json") as f:
        launches = json.load(f)
        image_urls = [launch["image"] for launch in launches["results"]]
        for image_url in image_urls:
            try:
                response = requests.get(image_url)
                image_filename = image_url.split("/")[-1]
                target_file = f"/tmp/images/{image_filename}"
                with open(target_file, "wb") as f:
                    f.write(response.content)
                print(f"Downloaded {image_url} to {target_file}")
            except requests_exceptions.MissingSchema:
                print(f"{image_url} appears to be an invalid URL.")
            except requests_exceptions.ConnectionError:
                print(f"Could not connect to {image_url}.")


get_pictures = PythonOperator(
    task_id="get_pictures", python_callable=_get_pictures, dag=dag
)

notify = BashOperator(
    task_id="notify",
    bash_command='echo "There are now $(ls /tmp/images/ | wc -l) images."',
    dag=dag,
)

download_launches >> get_pictures >> notify
```

The DAG class takes two required arguments: `dag_id` and `start_date`.

- `dag_id`: The name of the DAG displayed in the Airflow user interface (UI).
- `start_date`: The datetime at which the workflow should first start running.

Also note we set `schedule_interval` to `None`. This means the DAG will not run
automatically. For now, you can trigger it manually from the Airflow UI.

Each operator performs a single unit of work, and multiple operators together
form a workflow or DAG in Airflow.

The `BashOperator` arguments:

- `task_id`: The name of the task.
- `bash_command`: The Bash command to execute.
- `dag`: Reference to the DAG variable.

In Airflow, we can use the _binary right shift operator_ (i.e., “_rshift_”
[`>>`]) to define dependencies between tasks.

##### Tasks vs. operators

In Airflow, _operators_ have a single piece of responsibility: they exist to
perform one single piece of work. Some operators perform generic work, such as
the `BashOperator` (used to run a Bash script) or the `PythonOperator` (used to
run a Python function); others have more specific use cases, such as the
`EmailOperator` (used to send an email) or the `SimpleHTTPOperator` (used to
call an HTTP endpoint).

Tasks in Airflow manage the execution of an operator; they can be thought of as
a small wrapper or manager around an operator that ensures the operator executes
correctly. The user can focus on the work to be done by using operators, while
Airflow ensures correct execution of the work via tasks.

<Figure src="/media/airflow-tasks-vs-operators.png">
  DAGs and operators are used by Airflow users. Tasks are internal components to
  manage operator state and display state changes (e.g., started/finished) to
  the user.
</Figure>

##### Running arbitrary Python code

The `PythonOperator` in Airflow is responsible for running any Python code. Just
like the `BashOperator` used before, this and all other operators require a
`task_id`. The `python_callable` argument points to a callable, typically a
function.

#### Running a DAG in Airflow

##### Running Airflow in a Python environment

```shell
pip install apache-airflow
```

After installing Airflow, start it by initializing the metastore (a database in
which all Airflow state is stored), creating a user, copying the rocket launch
DAG into the DAGs directory, and starting the scheduler and webserver:

1. ```shell
   airflow db init
   ```
2. ```shell
   airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org
   ```
3. ```shell
   cp download_rocket_launches.py ~/airflow/dags/
   ```
4. ```shell
   airflow webserver
   ```
5. ```shell
   airflow scheduler
   ```

##### Running Airflow in Docker containers

```shell
docker run \
    -ti \
    -p 8080:8080 \
    -v /path/to/dag/download_rocket_launches.py:/opt/airflow/dags/download_rocket_launches.py \
    --entrypoint=/bin/bash \
    --name airflow \
    apache/airflow:latest \
    -c '( \
            airflow db init && \
            airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org \
        ); \
        airflow webserver & \
        airflow scheduler \
       '
```

**Note:** In a production setting, you should run the Airflow webserver,
scheduler, and metastore in separate containers.

##### Inspecting the Airflow UI

<Figure src="/media/airflow-graph-view-2.png">Airflow graph view.</Figure>

First, the DAG needs to be “on” in order to be run.

#### Running at regular intervals

In Airflow, we can schedule a DAG to run at certain intervals, for example once
an hour, day, or month. This is controlled on the DAG by setting the
`schedule_interval` argument.

```python fp=download_rocket_launches.py hl=6
...

dag = DAG(
    dag_id="download_rocket_launches",
    start_date=airflow.utils.dates.days_ago(14),
    schedule_interval="@daily",
)

...
```

When we set the `schedule_interval` to `@daily`, Airflow knew it had to run this
DAG once a day. Given the `start_date` provided to the DAG of 14 days ago, that
means the time from 14 days ago up to now can be divided into 14 equal intervals
of one day. Since both the start and end date of these 14 intervals lie in the
past, they will start running once we provide a `schedule_interval` to Airflow.

#### Handling failing tasks

**Note:** It is unnecessary to restart the entire workflow. A nice feature of
Airflow is that you can restart from the point of failure and onward, without
having to restart any previously succeeded tasks.

Click the failed task, and then click the <Scr>Clear</Scr> button in the pop-up
(figure 2.17). It will show you the tasks you’re about to clear, meaning you
will reset the state of these tasks and Airflow will rerun them,

Click <Scr>OK!</Scr> and the failed task and its successive tasks will be
cleared.

Assuming the connectivity issues are resolved, the tasks will now run
successfully and make the whole tree view green.

After clearing the failed tasks, Airflow will automatically rerun these tasks.

### Scheduling in Airflow

#### An example: Processing user events

```python nu hl=18..20,27..30,42
from datetime import datetime
from pathlib import Path

import pandas as pd
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

dag = DAG(
    dag_id="01_unscheduled",
    start_date=datetime(2019, 1, 1),
    schedule_interval=None
)

fetch_events = BashOperator(
    task_id="fetch_events",
    bash_command=(
        "mkdir -p /data/events && "
        "curl -o /data/events.json "
        "http://events_api:5000/events"
    ),
    dag=dag,
)

def _calculate_stats(input_path, output_path):
    """Calculates event statistics."""
    events = pd.read_json(input_path)
    stats = events.groupby(["date", "user"]).size().reset_index()
    Path(output_path).parent.mkdir(exist_ok=True)
    stats.to_csv(output_path, index=False)

calculate_stats = PythonOperator(
    task_id="calculate_stats",
    python_callable=_calculate_stats,
    op_kwargs={
      "input_path": "/data/events.json",
      "output_path": "/data/stats.csv"
    },
    dag=dag,
)

fetch_events >> calculate_stats
```

#### Running at regular intervals

By default, the value of the `schedule_interval` argument is `None`, which means
the DAG will not be scheduled and will be run only when triggered manually from
the UI or the API.

##### Defining scheduling intervals

Airflow provides the convenient macro `@daily` for defining a daily scheduled
interval, which runs our DAG once every day at midnight.

Airflow also needs to know when we want to start executing the DAG, specified by
its start date. Based on this start date, Airflow will schedule the first
execution of our DAG to run at the first schedule interval _after_ the start
date (start + interval). Subsequent runs will continue executing at schedule
intervals following this first interval.

**Note:** Pay attention to the fact that Airflow starts tasks in an interval at
the end of the interval. If developing a DAG on January 1, 2019 at 13:00, with a
`start_date` of 01-01-2019 and `@daily` interval, this means it first starts
running at midnight. At first, nothing will happen if you run the DAG on January
1 at 13:00 until midnight is reached.

If we already know that our project has a fixed duration, we can tell Airflow to
stop running our DAG after a certain date using the `end_date` parameter.

##### Cron-based intervals

To support more complicated scheduling intervals, Airflow allows us to define
scheduling intervals using the same syntax as used by cron, a time-based job
scheduler used by Unix-like computer operating systems such as macOS and Linux.
This syntax consists of five components and is defined as follows:

```text
# ┌─────── minute (0 - 59)
# │ ┌────── hour (0 - 23)
# │ │ ┌───── day of the month (1 - 31)
# │ │ │ ┌───── month (1 - 12)
# │ │ │ │ ┌──── day of the week (0 - 6) (Sunday to Saturday;
# │ │ │ │ │ 7 is also Sunday on some systems)
# * * * * *
```

Asterisks (`*`) can be used instead of numbers to define unrestricted fields,
meaning we don’t care about the value of that field.

Examples:

- `0 * * * *` = hourly (running on the hour)
- `0 0 * * *` = daily (running at midnight)
- `0 0 * * 0` = weekly (running at midnight on Sunday)
- `0 0 1 * *` = midnight on the first of every month
- `45 23 * * SAT` = 23:45 every Saturday

Additionally, cron expressions allow you to define collections of values using a
comma (`,`) to define a list of values or a dash (`-`) to define a range of
values.

Examples:

- `0 0 * * MON,WED,FRI` = run every Monday, Wednesday, Friday at midnight
- `0 0 * * MON-FRI` = run every weekday at midnight
- `0 0,12 * * *` = run every day at 00:00 and 12:00

Airflow also provides support for several macros that represent shorthand for
commonly used scheduling intervals:

- `@once`: Schedule once and only once.
- `@hourly`: Run once an hour at the beginning of the hour.
- `@daily`: Run once a day at midnight.
- `@weekly`: Run once a week at midnight on Sunday morning.
- `@monthly`: Run once a month at midnight on the first day of the month.
- `@yearly`: Run once a year at midnight on January 1.

##### Frequency-based intervals

An important limitation of cron expressions is that they are unable to represent
certain frequency-based schedules. For example, how would you define a cron
expression that runs a DAG once every three days?

This limitation of cron stems from the nature of cron expressions, as they
define a pattern that is continuously matched against the current time to
determine whether a job should be executed. This has the advantage of making the
expressions stateless, meaning that you don’t have to remember when a previous
job was run to calculate the next interval. However, as you can see, this comes
at the price of some expressiveness. To use such a frequency-based schedule, you
can pass a `timedelta` instance (from the `datetime` module in the standard
library) as a schedule interval.

```python hl=3
dag = DAG(
    dag_id="04_time_delta",
    schedule_interval=dt.timedelta(days=3),
    start_date=dt.datetime(year=2019, month=1, day=1),
    end_date=dt.datetime(year=2019, month=1, day=5),
)
```

Of course, you can also use this approach to run your DAG every 10 minutes
(using `timedelta(minutes=10)`) or every two hours (using `timedelta(hours=2)`).

#### Processing data incrementally

Although we now have our DAG running at a daily interval (assuming we stuck with
the `@daily` schedule), we haven’t quite achieved our goal. For one, our DAG is
downloading and calculating statistics for the entire catalog of user events
every day, which is hardly efficient. Moreover, this process is only downloading
events for the past 30 days, which means we are not building any history for
earlier dates.

##### Fetching events incrementally

```shell
curl -O http:/ /localhost:5000/events?start_date=2019-01-01&end_date=2019-01-02
```

Note that in this example `start_date` is inclusive, while `end_date` is
exclusive, meaning we are effectively fetching events that occur between
2019-01-01 00:00:00 and 2019-01-01 23:59:59.

##### Dynamic time references using execution dates

Airflow provides tasks with extra parameters. The most important of these
parameters is called the `execution_date`, which represents the date and time
for which our DAG is being executed. The end time of the schedule interval is
indicated by another parameter called the `next_execution_date`. Airflow also
provides a previous_execution_date parameter, which describes the start of the
previous schedule interval.

<Figure src="/media/airflow-execution-dates.png">
  Execution dates in Airflow.
</Figure>

In Airflow, we can use these execution dates by referencing them in our
operators. For example, in the `BashOperator`, we can use Airflow’s Jinja-based
templating functionality to include the execution dates dynamically in our Bash
command.

```python nu hl=7..8
fetch_events = BashOperator(
    task_id="fetch_events",
    bash_command=(
        "mkdir -p /data && "
        "curl -o /data/events.json "
        "http://events_api:5000/events?"
        "start_date={{execution_date.strftime('%Y-%m-%d')}}&"
        "end_date={{next_execution_date.strftime('%Y-%m-%d')}}"
    ),
    dag=dag,
)
```

Airflow also provides several shorthand parameters for common date formats. For
example, the `ds` and `ds_nodash` parameters are different representations of
the `execution_date`, formatted as `YYYY-MM-DD` and `YYYYMMDD`, respectively.
Similarly, `next_ds`, `next_ds_nodash`, `prev_ds`, and `prev_ds_nodash` provide
shorthand notations for the next and previous execution dates, respectively.

```python hl=7..8
fetch_events = BashOperator(
    task_id="fetch_events",
    bash_command=(
        "mkdir -p /data && "
        "curl -o /data/events.json "
        "http://events_api:5000/events?"
        "start_date={{ds}}&"
        "end_date={{next_ds}}"
    ),
    dag=dag,
)
```

##### Partitioning your data

```python hl=5
fetch_events = BashOperator(
    task_id="fetch_events",
    bash_command=(
        "mkdir -p /data/events && "
        "curl -o /data/events/{{ds}}.json "
        "http://events_api:5000/events?"
        "start_date={{ds}}&"
        "end_date={{next_ds}}"
    ),
    dag=dag,
)
```

This practice of dividing a data set into smaller, more manageable pieces is a
common strategy in data storage and processing systems and is commonly referred
to as _partitioning_, with the smaller pieces of a data set the _partitions_.

```python nu hl=1,3..4,17..18
def _calculate_stats(**context):
    """Calculates event statistics."""
    input_path = context["templates_dict"]["input_path"]
    output_path = context["templates_dict"]["output_path"]

    events = pd.read_json(input_path)
    stats = events.groupby(["date", "user"]).size().reset_index()

    Path(output_path).parent.mkdir(exist_ok=True)
    stats.to_csv(output_path, index=False)


calculate_stats = PythonOperator(
    task_id="calculate_stats",
    python_callable=_calculate_stats,
    templates_dict={
        "input_path": "/data/events/{{ds}}.json",
        "output_path": "/data/stats/{{ds}}.csv",
    },
    dag=dag,
)
```

To achieve this templating in the `PythonOperator`, we need to pass any
arguments that should be templated using the operator’s `templates_dict`
parameter. We then can retrieve the templated values inside our function from
the context object that is passed to our `_calculate_stats` function by Airflow.

#### Understanding Airflow's execution dates

##### Executing work in fixed-length intervals

Airflow runs a DAG with three parameters: a start date, a schedule interval, and
an (optional) end date. Airflow uses these three parameters to divide time into
a series of schedule intervals, starting from the given start date and
optionally ending at the end date

<Figure src="/media/airflow-scheduling-intervals.png">
  Time represented in terms of Airflow’s scheduling intervals. Assumes a daily
  interval with a start date of 2019-01-01.
</Figure>

A DAG is executed for a given interval as soon as the time slot of that interval
has passed.

An advantage of using this interval-based approach is that it is ideal for
performing the type of incremental data processing we saw in the previous
sections, as we know exactly for which interval of time a task is executing
for—the start and end of the corresponding interval. This is in stark contrast
to, for example, a time point–based scheduling system such as cron, where we
only know the current time for which our task is being executed.

<Figure src="/media/interval-based-vs-time-point-based-scheduling.png">
  Incremental processing in interval-based scheduling windows (e.g., Airflow)
  versus windows derived from time point–based systems (e.g., cron). For
  incremental (data) processing, time is typically divided into discrete time
  intervals that are processed as soon as the corresponding interval has passed.
  Interval-based scheduling approaches (such as Airflow) explicitly schedule
  tasks to run for each interval while providing exact information to each task
  concerning the start and the end of the interval. In contrast, time
  point–based scheduling approaches only execute tasks at a given time, leaving
  it up to the task itself to determine for which incremental interval the task
  is executing.
</Figure>

Airflow defines the execution date of a DAG as the start of the corresponding
interval. Conceptually, this makes sense if we consider that the execution date
marks our schedule interval rather than the moment our DAG is actually executed.

When executing a task, the start and end of the corresponding interval are
defined by the `execution_date` (the start of the interval) and the
`next_execution` date (the start of the next interval) parameters. Similarly,
the previous schedule interval can be derived using the
`previous_execution_date` and `execution_date` parameters.

However, one caveat to keep in mind when using the `previous_execution_date` and
`next_execution_date` parameters in your tasks is that these are only defined
for DAG runs following the schedule interval. As such, the values of these
parameters will be undefined for any runs that are triggered manually using
Airflow UI or CLI because Airflow cannot provide information about next or
previous schedule intervals if you are not following a schedule interval.

#### Using backfilling to fill in past gaps

##### Executing work back in time

By default, Airflow will schedule and run any past schedule intervals that have
not been run. As such, specifying a past start date and activating the
corresponding DAG will result in all intervals that have passed before the
current time being executed. This behavior is controlled by the DAG `catchup`
parameter and can be disabled by setting `catchup` to false.

```python hl=6
dag = DAG(
    dag_id="09_no_catchup",
    schedule_interval="@daily",
    start_date=dt.datetime(year=2019, month=1, day=1),
    end_date=dt.datetime(year=2019, month=1, day=5),
    catchup=False,
)
```

With this setting, the DAG will only be run for the most recent schedule
interval rather than executing all open past intervals. The default value for
catchup can be controlled from the Airflow configuration file by setting a value
for the `catchup_by_default` configuration setting.

<Figure src="/media/airflow-catchup-parameter.png">

Backfilling in Airflow. By default, Airflow will run tasks for all past
intervals up to the current time. This behavior can be disabled by setting the
`catchup` parameter of a DAG to false, in which case Airflow will only start
executing tasks from the current interval.

</Figure>

Although backfilling is a powerful concept, it is limited by the availability of
data in source systems. For example, in our example use case we can load past
events from our API by specifying a start date up to 30 days in the past.
However, as the API only provides up to 30 days of history, we cannot use
backfilling to load data from earlier days.

Backfilling can also be used to reprocess data after we have made changes in our
code. For example, say we make a change to our `calc_statistics` function to add
a new statistic. Using backfilling, we can clear past runs of our
`calc_statistics` task to reanalyze our historical data using the new code. Note
that in this case we aren’t limited by the 30-day limit of our data source, as
we have already loaded these earlier data partitions as part of our past runs.

#### Best practices for designing tasks

### Templating tasks using the Airflow context

#### Inspecting data for processing with Airflow

#### Task context and Jinja templating

#### Hooking up other systems

### Defining dependencies between tasks

#### Basic dependencies

#### Branching

#### Conditional tasks

#### More about trigger rules

#### Sharing data between tasks

#### Chaining Python tasks with the Taskflow API

## Beyong the basics

### Triggering workflows

#### Polling conditions with sensors

#### Triggering other DAGs

#### Starting workflows with REST/CLI

### Communicating with external systems

#### Connecting to cloud services

#### Moving data from between systems

### Building custom components

#### Starting with a PythonOperator

#### Building a custom hook

#### Building a custom operator

#### Building custom sensors

#### Packaging your components

### Testing

#### Getting started with testing

#### Working with DAGs and task context in tests

#### Using tests from development

#### Emulate production environments with Whirl

#### Create DTAP environments

### Running tasks in containers

#### Challenges of many different operators

#### Introducing containers

#### Containers and Airflow

#### Running tasks in Docker

#### Running tasks in Kubernetes

### Airflow in practice

#### Best practices

#### Writing clean DAGs

#### Designing reproducible tasks

#### Handling data efficiently

#### Managing your resources

### Operating Airflow in production

#### Airflow architectures

#### Installing each executor

#### Capturing logs of all Airflow processes

#### Visualizing and monitoring Airflow metrics

#### How to get notified of a failing task

### Securing Airflow

#### Securing the Airflow web interface

#### Encrypting data at rest

#### Connecting with an LDAP service

#### Encrypting traffic to the webserver

#### Fetching credentials from secret management

### Project: Finding the fastest way to get around NYC

#### Understanding the data

#### Extracting the data

#### Applying similar transformations to data

#### Structuring a data pipeline

#### Developing idempotent data pipelines

## In the clouds

### Airflow in the clouds

#### Designing (cloud) deployment strategies

#### Cloud-specific operators and hooks

#### Managed services

#### Choosing a deployment strategy

### Airflow on AWS

#### Deploying Airflow in AWS

#### AWS-specific hooks and operators

#### Use case: Serverless movie ranking with AWS Athena

### Airflow on Azure

#### Deploying Airflow in Azure

#### Azure-specific hooks/operators

#### Example: Serverless movie ranking with Azure

### Airflow in GCP

#### Deploying Airflow in GCP

#### GCP-specific hooks and operators

#### Use case: Serverless movie ranking on GCP
