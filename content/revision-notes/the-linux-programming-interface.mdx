---
title: The Linux Programming Interface
resourceId: "9781593272203"
stoppedAt: Interprocess Communication and Synchronization
---

## History and Standards

Two definitions of the term UNIX are in common use. One of these denotes
operating systems that have passed the official conformance tests for the Single
UNIX Specification and thus are officially granted the right to be branded as
“UNIX” by The Open Group (the holders of the UNIX trademark). At the time of
writing, none of the free UNIX implementations (e.g., Linux and FreeBSD) has
obtained this branding.

The other common meaning attached to the term UNIX denotes those systems that
look and behave like classical UNIX systems (i.e., the original Bell
Laboratories UNIX and its later principal offshoots, System V and BSD). By this
definition, Linux is generally considered to be a UNIX system (as are the modern
BSDs). Although we give close attention to the Single UNIX Specification in this
book, we’ll follow this second definition of UNIX, so that we’ll often say
things such as “Linux, like other UNIX implementations....”

### A Brief History of UNIX and C

The first UNIX implementation was developed in 1969 (the same year that Linus
Torvalds was born) by Ken Thompson at Bell Laboratories, a division of the
telephone corporation, AT&T.

By 1973, C had matured to a point where the UNIX kernel could be almost entirely
rewritten in the new language. UNIX thus became one of the earliest operating
systems to be written in a high-level language, a fact that made subsequent
porting to other hardware architectures possible.

Previous widely used languages were designed with other purposes in mind:
FORTRAN for mathematical tasks performed by engineers and scientists; COBOL for
commercial systems processing streams of record-oriented data. C filled a
hitherto empty niche, and unlike FORTRAN and COBOL (which were designed by large
committees), the design of C arose from the ideas and needs of a few individuals
working toward a single goal: developing a high-level language for implementing
the UNIX kernel and associated software.

#### UNIX First through Sixth editions

Between 1969 and 1979, UNIX went through a number of releases, known as
editions. Essentially, these releases were snapshots of the evolving development
version at AT&T.

Over the period of these releases, the use and reputation of UNIX began to
spread, first within AT&T, and then beyond.

At this time, AT&T held a government-sanctioned monopoly on the US telephone
system. The terms of AT&T’s agreement with the US government prevented it from
selling software, which meant that it could not sell UNIX as a product.

AT&T licensed UNIX for use in universities for a nominal distribution fee. The
university distributions included documentation and the kernel source code
(about 10,000 lines at the time).

AT&T’s release of UNIX into universities greatly contributed to the popularity
and use of the operating system, and by 1977, UNIX was running at some 500
sites, including 125 universities in the United States and several other
countries. UNIX offered universities an interactive multiuser operating system
that was cheap yet powerful, at a time when commercial operating systems were
very expensive. It also gave university computer science departments the source
code of a real operating system, which they could modify and offer to their
students to learn from and experiment with. Some of these students, armed with
UNIX knowledge, became UNIX evangelists.

#### The birth of BSD and System V

January 1979 saw the release of Seventh Edition UNIX. The release of Seventh
Edition is also significant because, from this point, UNIX diverged into two
important variants: BSD and System V.

Thompson spent the 1975/1976 academic year as a visiting professor at the
University of California at Berkeley, the university from which he had
graduated. There, he worked with several graduate students, adding many new
features to UNIX. (One of these students, Bill Joy, subsequently went on to
cofound Sun Microsystems, an early entry in the UNIX workstation market.) Over
time, many new tools and features were developed at Berkeley, including the _C
shell_, the _vi_ editor, an improved file system (the _Berkeley Fast File
System_), _sendmail_, a Pascal compiler, and virtual memory management on the
new Digital VAX architecture.

Under the name Berkeley Software Distribution (BSD), this version of UNIX,
including its source code, came to be widely distributed.

In 1983, the _Computer Systems Research Group_ at the University of California
at Berkeley released 4.2BSD. This release was significant because it contained a
complete TCP/IP implementation, including the sockets application programming
interface (API) and a variety of networking tools. 4.2BSD and its predecessor
4.1BSD became widely distributed within universities around the world. They also
formed the basis for SunOS (first released in 1983), the UNIX variant sold by
Sun.

In the meantime, US antitrust legislation forced the breakup of AT&T (legal
maneuvers began in the mid-1970s, and the breakup became effective in 1982),
with the consequence that, since it no longer held a monopoly on the telephone
system, the company was permitted to market UNIX. This resulted in the release
of System III (three) in 1981. System III was produced by AT&T’s UNIX Support
Group (USG), which employed many hundreds of developers to enhance UNIX and
develop UNIX applications (notably, document preparation packages and software
development tools). The first release of System V (five) followed in 1983, and a
series of releases led to the definitive System V Release 4 (SVR4) in 1989, by
which time System V had incorporated many features from BSD, including
networking facilities. System V was licensed to a variety of commercial vendors,
who used it as the basis of their UNIX implementations.

Thus, in addition to the various BSD distributions spreading through academia,
by the late 1980s, UNIX was available in a range of commercial implementations
on various hardware. These implementations included Sun’s SunOS and later
Solaris, Digital’s Ultrix and OSF/1 (nowadays, after a series of renamings and
acquisitions, HP Tru64 UNIX), IBM’s AIX, Hewlett-Packard’s (HP’s) HP-UX, NeXT’s
NeXTStep, A/UX for the Apple Macintosh, and Microsoft and SCO’s XENIX for the
Intel x86-32 architecture. (Throughout this book, the Linux implementation for
x86-32 is referred to as Linux/x86-32.)

### A Brief History of Linux

The term _Linux_ is commonly used to refer to the entire UNIX-like operating
system of which the Linux kernel forms a part. However, this is something of a
misnomer, since many of the key components contained within a typical commercial
Linux distribution actually originate from a project that predates the inception
of Linux by several years.

#### The GNU Project

In 1984, Richard Stallman, an exceptionally talented programmer who had been
working at MIT, set to work on creating a “free” UNIX implementation.

Stallman started the GNU project (a recursively defined acronym for “GNU’s not
UNIX”) to develop an entire, freely available, UNIX-like system, consisting of a
kernel and all associated software packages, and encouraged others to join him.
In 1985, Stallman founded the Free Software Foundation (FSF), a nonprofit
organization to support the GNU project as well as the development of free
software in general.

When the GNU project was started, BSD was not free in the sense that Stallman
meant. Use of BSD still required a license from AT&T, and users could not freely
modify and redistribute the AT&T code that formed part of BSD.

One of the important results of the GNU project was the development of the GNU
_General Public License_ (GPL), the legal embodiment of Stallman’s notion of
free software. Much of the software in a Linux distribution, including the
kernel, is licensed under the GPL or one of a number of similar licenses.
Software licensed under the GPL must be made available in source code form, and
must be freely redistributable under the terms of the GPL. Modifications to
GPL-licensed software are freely permitted, but any distribution of such
modified software must also be under the terms of the GPL. If the modified
software is distributed in executable form, the author must also allow any
recipients the option of obtaining the modified source for no more than the cost
of distribution.

The GNU project did not initially produce a working UNIX kernel, but did produce
a wide range of other programs. Since these programs were designed to run on a
UNIX-like operating system, they could be, and were, used on existing UNIX
implementations and, in some cases, even ported to other operating systems.
Among the more well-known programs produced by the GNU project are the _Emacs_
text editor, _GCC_ (originally the GNU C compiler, but now renamed the GNU
compiler collection, comprising compilers for C, C++, and other languages), the
_bash_ shell, and _glibc_ (the GNU C library).

By the early 1990s, the GNU project had produced a system that was virtually
complete, except for one important component: a working UNIX kernel. The GNU
project had started work on an ambitious kernel design, known as the GNU/HURD,
based on the Mach microkernel. However, the HURD was far from being in a form
that could be released. (At the time of writing, work continues on the HURD,
which currently runs only on the x86-32 architecture.)

**Note:** Because a significant part of the program code that constitutes what
is commonly known as the Linux system actually derives from the GNU project,
Stallman prefers to use the term _GNU/Linux_ to refer to the entire system. The
question of naming (Linux versus GNU/Linux) is the source of some debate in the
free software community. Since this book is primarily concerned with the API of
the Linux kernel, we’ll generally use the term _Linux_.

#### The Linux Kernel

In 1991, Linus Torvalds, a Finnish student at the University of Helsinki, was
inspired to write an operating system for his Intel 80386 PC. In the course of
his studies, Torvalds had come into contact with Minix, a small UNIX-like
operating system kernel developed in the mid-1980s by Andrew Tanenbaum, a
university professor in Holland. Tanenbaum made Minix, complete with source
code, available as a tool for teaching operating system design in university
courses. The Minix kernel could be built and run on a 386 system. However, since
its primary purpose was as a teaching tool, it was designed to be largely
independent of the hardware architecture, and it did not take full advantage of
the 386 processor’s capabilities.

Torvalds therefore started on a project to create an efficient, full-featured
UNIX kernel to run on the 386. Over a few months, Torvalds developed a basic
kernel that allowed him to compile and run various GNU programs. Then, on
October 5, 1991, Torvalds requested the help of other programmers.

The call for support proved effective. Other programmers joined Torvalds in the
development of Linux, adding various features, such as an improved file system,
networking support, device drivers, and multiprocessor support. By March 1994,
the developers were able to release version 1.0.

##### An aside: the BSDs

It is worth noting that another free UNIX was already available for the x86-32
during the early 1990s. Bill and Lynne Jolitz had developed a port of the
already mature BSD system for the x86-32, known as 386/BSD. This port was based
on the BSD Net/2 release ( June 1991), a version of the 4.3BSD source code in
which all remaining proprietary AT&T source code had either been replaced or, in
the case of six source code files that could not be trivially rewritten,
removed. The Jolitzes ported the Net/2 code to x86-32, rewrote the missing
source files, and made the first release (version 0.0) of 386/BSD in
February 1992.

After an initial wave of success and popularity, work on 386/BSD lagged for
various reasons. In the face of an increasingly large backlog of patches, two
alternative development groups soon appeared, creating their own releases based
on 386/BSD: NetBSD, which emphasizes portability to a wide range of hardware
platforms, and FreeBSD, which emphasizes performance and is the most widespread
of the modern BSDs. The first NetBSD release was 0.8, in April 1993. The first
FreeBSD CD-ROM (version 1.0) appeared in December 1993. Another BSD, OpenBSD,
appeared in 1996 (as an initial version numbered 2.0) after forking from the
NetBSD project. OpenBSD emphasizes security. In mid-2003, a new BSD, DragonFly
BSD, appeared after a split from FreeBSD 4.x. DragonFly BSD takes a different
approach from FreeBSD 5.x with respect to design for symmetric multiprocessing
(SMP) architectures.

Probably no discussion of the BSDs in the early 1990s is complete without
mention of the lawsuits between UNIX System Laboratories (USL, the AT&T
subsidiary spun off to develop and market UNIX) and Berkeley. In early 1992, the
company Berkeley Software Design, Incorporated (BSDi, nowadays part of Wind
River) began distributing a commercially supported BSD UNIX, BSD/OS, based on
the Net/2 release and the Jolitzes’ 386/BSD additions. BSDi distributed binaries
and source code for $995 (US dollars), and advised potential customers to use
their telephone number 1-800-ITS-UNIX.

In April 1992, USL filed suit against BSDi in an attempt to prevent BSDi from
selling a product that USL claimed was still encumbered by proprietary USL
source code and trade secrets. USL also demanded that BSDi cease using the
deceptive telephone number. The suit was eventually widened to include a claim
against the University of California. The court ultimately dismissed all but two
of USL’s claims, and a countersuit by the University of California against USL
ensued, in which the university claimed that USL had not given due credit for
the use of BSD code in System V.

While these suits were pending, USL was acquired by Novell, whose CEO, the late
Ray Noorda, stated publicly that he would prefer to compete in the marketplace
rather than in the court. Settlement was finally reached in January 1994, with
the University of California being required to remove 3 of the 18,000 files in
the Net/2 release, make some minor changes to a few other files, and add USL
copyright notices to around 70 other files, which the university nevertheless
could continue to distribute freely. This modified system was released as
4.4BSD-Lite in June 1994. (The last release from the university was 4.4BSD-Lite,
Release 2 in June 1995.) At this point, the terms of the legal settlement
required BSDi, FreeBSD, and NetBSD to replace their Net/2 base with the modified
4.4BSD-Lite source code. As [McKusick et al., 1996] notes, although this caused
some delay in the development of the BSD derivatives, it also had the positive
effect that these systems resynchronized with the three years of development
work done by the university’s Computer Systems Research Group since the release
of Net/2.

##### Linux kernel version numbers

Like most free software projects, Linux follows a release-early, release-often
model, so that new kernel revisions appear frequently (sometimes even daily). As
the Linux user base increased, the release model was adapted to decrease
disruption to existing users. Specifically, following the release of Linux 1.0,
the kernel developers adopted a kernel version numbering scheme with each
release numbered _x.y.z_: _x_ representing a major version, _y_ a minor version
within that major version, and _z_ a revision of the minor version (minor
improvements and bug fixes).

Under this model, two kernel versions were always under development: a _stable_
branch for use on production systems, which had an even minor version number,
and a more volatile _development_ branch, which carried the next higher odd
minor version number. The theory—not always followed strictly in practice—was
that all new features should be added in the current development kernel series,
while new revisions in the stable kernel series should be restricted to minor
improvements and bug fixes. When the current development branch was deemed
suitable for release, it became the new stable branch and was assigned an even
minor version number. For example, the 2.3._z_ development kernel branch
resulted in the 2.4 stable kernel branch.

Following the 2.6 kernel release, the development model was changed. The main
motivation for this change arose from problems and frustrations caused by the
long intervals between stable kernel releases. (Nearly three years passed
between the release of Linux 2.4.0 and 2.6.0.) There have periodically been
discussions about fine-tuning this model, but the essential details have
remained as follows:

- There is no longer a separation between stable and development kernels. Each
  new 2.6._z_ release can contain new features, and goes through a life cycle
  that begins with the addition of new features, which are then stabilized over
  the course of a number of candidate release versions. When a candidate version
  is deemed sufficiently stable, it is released as kernel 2.6._z_. Release
  cycles are typically about three months long.
- Sometimes, a stable 2.6._z_ release may require minor patches to fix bugs or
  security problems. If these fixes have a sufficiently high priority, and the
  patches are deemed simple enough to be “obviously” correct, then, rather than
  waiting for the next 2.6._z_ release, they are applied to create a release
  with a number of the form 2.6._z_._r_, where r is a sequential number for a
  minor revision of this 2.6._z_ kernel.
- Additional responsibility is shifted onto distribution vendors to ensure the
  stability of the kernel provided with a distribution.

##### Ports to other hardware architecture

During the initial development of Linux, efficient implementation on the Intel
80386 was the primary goal, rather than portability to other processor
architectures. However, with the increasing popularity of Linux, ports to other
processor architectures began to appear, starting with an early port to the
Digital Alpha chip. The list of hardware architectures to which Linux has been
ported continues to grow

##### Linux distributions

Precisely speaking, the term _Linux_ refers just to the kernel developed by
Linus Torvalds and others. However, the term _Linux_ is commonly used to mean
the kernel, plus a wide range of other software (tools and libraries) that
together make a complete operating system. In the very early days of Linux, the
user was required to assemble all of this software, create a file system, and
correctly place and configure all of the software on that file system. This
demanded considerable time and expertise. As a result, a market opened for Linux
distributors, who created packages (_distributions_) to automate most of the
installation process, creating a file system and installing the kernel and other
required software.

The earliest distributions appeared in 1992, and included MCC Interim Linux
(Manchester Computing Centre, UK), TAMU (Texas A&M University), and SLS
(SoftLanding Linux System). The oldest surviving commercial distribution,
Slackware, appeared in 1993. The noncommercial Debian distribution appeared at
around the same time, and SUSE and Red Hat soon followed. The currently very
popular Ubuntu distribution first appeared in 2004. Nowadays, many distribution
companies also employ programmers who actively contribute to existing free
software projects or initiate new projects.

### Standardization

By the late 1980s, the wide variety of available UNIX implementations also had
its drawbacks. Some UNIX implementations were based on BSD, others were based on
System V, and some drew features from both variants. Furthermore, each
commercial vendor had added extra features to its own implementation. The
consequence was that moving software and people from one UNIX implementation to
another became steadily more difficult. This situation created strong pressure
for standardization of the C programming language and the UNIX system, so that
applications could more easily be ported from one system to another.

#### The C Programming Language

Minor differences had arisen between the various implementations, in part
because certain aspects of how the language should function were not detailed in
the existing de facto standard for C, Kernighan and Ritchie’s 1978 book, _The C
Programming Language_. (The older C syntax described in that book is sometimes
called traditional C or K&R C.)

These factors created a drive for C standardization that culminated in 1989 with
the approval of the American National Standards Institute (ANSI) C standard,
which was subsequently adopted in 1990 as an International Standards
Organization (ISO) standard. This version of C is usually known as C89 or (less
commonly) ISO C90, and is fully described in the second (1988) edition of
Kernighan and Ritchie’s _The C Programming Language_.

A revision of the C standard was adopted by ISO in 1999. This standard is
usually referred to as C99, and includes a range of changes to the language and
its standard library.

The C standards are independent of the details of any operating system; that is,
they are not tied to the UNIX system. This means that C programs written using
only the standard C library should be portable to any computer and operating
system providing a C implementation.

**Note:** Historically, C89 was often called ANSI C, and this term is sometimes
still used with that meaning. For example, _gcc_ employs that meaning; its
_–ansi_ qualifier means “support all ISO C90 programs.” However, we avoid this
term because it is now somewhat ambiguous. Since the ANSI committee adopted the
C99 revision, properly speaking, ANSI C is now C99.

#### The First POSIX Standards

The term POSIX (an abbreviation of _Portable Operating System Interface_) refers
to a group of standards developed under the auspices of the Institute of
Electrical and Electronic Engineers (IEEE), specifically its Portable
Application Standards Committee (PACS). The goal of the PASC standards is to
promote application portability at the source code level.

##### POSIX.1 and POSIX.2

POSIX.1 became an IEEE standard in 1988 and, with minor revisions, was adopted
as an ISO standard in 1990.

POSIX.1 documents an API for a set of services that should be made available to
a program by a conforming operating system. An operating system that does this
can be certified as _POSIX.1 conformant_.

POSIX.1 is based on the UNIX system call and the C library function API, but it
doesn’t require any particular implementation to be associated with this
interface. This means that the interface can be implemented by any operating
system, not specifically a UNIX operating system. In fact, some vendors have
added APIs to their proprietary operating systems that make them POSIX.1
conformant, while at the same time leaving the underlying operating system
largely unchanged.

A number of extensions to the original POSIX.1 standard were also important. A
related standard, POSIX.2, standardized the shell and various UNIX utilities,
including the command-line interface of the C compiler.

##### FIPS 151-1 and FIPS 151-2

FIPS is an abbreviation for Federal Information Processing Standard, the name of
a set of standards specified by the US government for the purchase of its
computer systems. In 1989, FIPS 151-1 was published. This standard was based on
the 1988 IEEE POSIX.1 standard and the draft ANSI C standard. The main
difference between FIPS 151-1 and POSIX.1 (1988) was that the FIPS standard
required some features that POSIX.1 left as optional. Because the US government
is a major purchaser of computer systems, most computer vendors ensured that
their UNIX systems conformed to the FIPS 151-1 version of POSIX.1.

FIPS 151-2 aligned with the 1990 ISO edition of POSIX.1, but was otherwise
unchanged. The now outdated FIPS 151-2 was withdrawn as a standard in
February 2000.

#### X/Open Company and The Open Group

X/Open Company was a consortium formed by an international group of computer
vendors to adopt and adapt existing standards in order to produce a
comprehensive, consistent set of open systems standards. It produced the _X/Open
Portability Guide_, a series of portability guides based on the POSIX standards.
The first important release of this guide was Issue 3 (XPG3) in 1989, followed
by XPG4 in 1992. XPG4 was revised in 1994, which resulted in XPG4 version 2.
This revision was also known as _Spec 1170_, with 1170 referring to the number
of _interfaces_—functions, header files, and commands— defined by the standard.

When Novell, which acquired the UNIX systems business from AT&T in early 1993,
later divested itself of that business, it transferred the rights to the UNIX
trademark to X/Open. XPG4 version 2 was subsequently repackaged as the _Single
UNIX Specification_ (SUS, or sometimes SUSv1), and is also known as _UNIX 95_.
This repackaging included XPG4 version 2, the X/Open Curses Issue 4 version 2
specification, and the X/Open Networking Services (XNS) Issue 4 specification.
Version 2 of the Single UNIX Specification (SUSv2) appeared in 1997, and UNIX
implementations certified against this specification can call themselves _UNIX
98_.

In 1996, X/Open merged with the _Open Software Foundation_ (OSF) to form _The
Open Group_. Nearly every company or organization involved with the UNIX system
is now a member of The Open Group, which continues to develop API standards.

**Note:** OSF was one of two vendor consortia formed during the UNIX wars of the
late 1980s. Among others, OSF included Digital, IBM, HP, Apollo, Bull, Nixdorf,
and Siemens. OSF was formed primarily in response to the threat created by a
business alliance between AT&T (the originators of UNIX) and Sun (the most
powerful player in the UNIX workstation market). Consequently, AT&T, Sun, and
other companies formed the rival _UNIX International_ consortium.

#### SUSv3 and POSIX.1-2001

Beginning in 1999, the IEEE, The Open Group, and the ISO/IEC Joint Technical
Committee 1 collaborated in the _Austin Common Standards Revision Group_ (CSRG)
with the aim of revising and consolidating the POSIX standards and the Single
UNIX Specification. This resulted in the ratification of POSIX 1003.1-2001,
sometimes just called POSIX.1-2001, in December 2001 (subsequently approved as
an ISO standard, ISO/IEC 9945:2002).

POSIX 1003.1-2001 replaces SUSv2, POSIX.1, POSIX.2, and a raft of other earlier
POSIX standards. This standard is also known as the Single UNIX Specification
Version 3, and we’ll generally refer to it as _SUSv3_.

The SUSv3 base specifications consists of around 3700 pages, divided into the
following four parts:

- _Base Definitions_ (XBD): This part contains definitions, terms, concepts, and
  specifications of the contents of header files. A total of 84 header file
  specifications are provided.
- _System Interfaces_ (XSH): This part begins with various useful background
  information. Its bulk consists of the specification of various functions
  (which are implemented as either system calls or library functions on specific
  UNIX implementations). A total of 1123 system interfaces are included in this
  part.
- _Shell and Utilities_ (XCU): This specifies the operation of the shell and
  various UNIX commands. A total of 160 utilities are specified in this part.
- _Rationale_ (XRAT): This part includes informative text and justifications
  relating to the earlier parts.

In addition, SUSv3 includes the _X/Open CURSES Issue 4 Version 2_ (XCURSES)
specification, which specifies 372 functions and 3 header files for the _curses_
screenhandling API.

In all, 1742 interfaces are specified in SUSv3. By contrast, POSIX.1-1990 (with
FIPS 151-2) specified 199 interfaces, and POSIX.2-1992 specified 130 utilities.

UNIX implementations certified against SUSv3 can call themselves _UNIX 03_.
There have been various minor fixes and improvements for problems discovered
since the ratification of the original SUSv3 text. These have resulted in the
appearance of _Technical Corrigendum Number 1_, whose improvements were
incorporated in a 2003 revision of SUSv3, and _Technical Corrigendum Number 2_,
whose improvements were incorporated in a 2004 revision.

##### POSIX conformance, XSI conformance, and the XSI extension

Historically, the SUS (and XPG) standards deferred to the corresponding POSIX
standards and were structured as functional supersets of POSIX. As well as
specifying additional interfaces, the SUS standards made mandatory many of the
interfaces and behaviors that were deemed optional in POSIX.

This distinction survives somewhat more subtly in POSIX 1003.1-2001, which is
both an IEEE standard and an Open Group Technical Standard. This document
defines two levels of conformance:

- _POSIX conformance:_ This defines a baseline of interfaces that a conforming
  implementation must provide. It permits the implementation to provide other
  optional interfaces.
- _X/Open System Interface (XSI) conformance_ To be XSI conformant, an
  implementation must meet all of the requirements of POSIX conformance and also
  must provide a number of interfaces and behaviors that are only optionally
  required for POSIX conformance. An implementation must reach this level of
  conformance in order to obtain the UNIX 03 branding from The Open Group.

The additional interfaces and behaviors required for XSI conformance are
collectively known as the _XSI extension_.

In later chapters, when we talk about SUSv3 conformance, we mean XSI
conformance.

##### Unspecified and weakly specified

Occasionally, we refer to an interface as being “unspecified” or “weakly
specified” within SUSv3.

By an _unspecified interface_, we mean one that is not defined at all in the
formal standard, although in a few cases there are background notes or rationale
text that mention the interface.

Saying that an interface is _weakly specified_ is shorthand for saying that,
while the interface is included in the standard, important details are left
unspecified (commonly because the committee members could not reach an agreement
due to differences in existing implementations).

When using interfaces that are unspecified or weakly specified, we have few
guarantees when porting applications to other UNIX implementations.
Nevertheless, in a few cases, such an interface is quite consistent across
implementations, and where this is so, we generally note it as such.

##### LEGACY features

Sometimes, we note that SUSv3 marks a specified feature as _LEGACY_. This term
denotes a feature that is retained for compatibility with older applications,
but whose limitations mean that its use should be avoided in new applications.
In many cases, some other API exists that provides equivalent functionality.

#### SUSv4 and POSIX.1-2008

In 2008, the Austin group completed a revision of the combined POSIX.1 and
Single UNIX Specification. As with the preceding version of the standard, it
consists of a base specification coupled with an XSI extension. We’ll refer to
this revision as SUSv4.

#### UNIX Standards Timeline

<Figure src="/media/relationships-unix-and-c-standards.png" alt="TODO">
  Relationships between various UNIX and C standards.
</Figure>

The situation with networking standards is somewhat complex. Standardization
efforts in this area began in the late 1980s with the formation of the POSIX
1003.12 committee to standardize the sockets API, the X/Open Transport Interface
(XTI) API (an alternative network programming API based on System V’s Transport
Layer Interface), and various associated APIs. The gestation of this standard
occurred over several years, during which time POSIX 1003.12 was renamed POSIX
1003.1g. It was ratified in 2000.

In parallel with the development of POSIX 1003.1g, X/Open was also developing
its X/Open Networking Specification (XNS). The first version of this
specification, XNS Issue 4, was part of the first version of the Single UNIX
Specification. It was succeeded by XNS Issue 5, which formed part of SUSv2. XNS
Issue 5 was essentially the same as the then current (6.6) draft of POSIX.1g.
This was followed by XNS Issue 5.2, which differed from XNS Issue 5 and the
ratified POSIX.1g standard in marking the XTI API as obsolete and in including
coverage of Internet Protocol version 6 (IPv6), which was being designed in the
mid-1990s). XNS Issue 5.2 formed the basis for the networking material included
in SUSv3, and is thus now superseded. For similar reasons, POSIX.1g was
withdrawn as a standard soon after it was ratified.

#### Implementation Standards

In addition to the standards produced by independent or multiparty groups,
reference is sometimes made to the two implementation standards defined by the
final BSD release (4.4BSD) and AT&T’s System V Release 4 (SVR4). The latter
implementation standard was formalized by AT&T’s publication of the System V
Interface Definition (SVID). In 1989, AT&T published Issue 3 of the SVID, which
defined the interface that a UNIX implementation must provide in order to be
able to call itself System V Release 4.

**Note:** Because the behavior of some system calls and library functions varies
between SVR4 and BSD, many UNIX implementations provide compatibility libraries
and conditional-compilation facilities that emulate the behavior of whichever
UNIX flavor is not used as the base for that particular implementation This
eases the burden of porting an application from another UNIX implementation.

#### Linux, Standards, and the Linux Standard Base

As a general goal, Linux (i.e., kernel, _glibc_, and tool) development aims to
conform to the various UNIX standards, especially POSIX and the Single UNIX
Specification. However, at the time of writing, no Linux distributions are
branded as “UNIX” by The Open Group. The problems are time and expense. Each
vendor distribution would need to undergo conformance testing to obtain this
branding, and it would need to repeat this testing with each new distribution
release. Nevertheless, it is the de facto near-conformance to various standards
that has enabled Linux to be so successful in the UNIX market. With most
commercial UNIX implementations, the same company both develops and distributes
the operating system. With Linux, things are different, in that implementation
is separate from distribution, and multiple organizations—both commercial and
noncommercial—handle Linux distribution.

Linus Torvalds doesn’t contribute to or endorse a particular Linux distribution.
However, in terms of other individuals carrying out Linux development, the
situation is more complex. Many developers working on the Linux kernel and on
other free software projects are employed by various Linux distribution
companies or work for companies (such as IBM and HP) with a strong interest in
Linux. While these companies can influence the direction in which Linux moves by
allocating programmer hours to certain projects, none of them controls Linux as
such. And, of course, many of the other contributors to the Linux kernel and GNU
projects work voluntarily.

Because there are multiple Linux distributors and because the kernel
implementers don’t control the contents of distributions, there is no “standard”
commercial Linux as such. Each Linux distributor’s kernel offering is typically
based on a snapshot of the mainline (i.e., the Torvalds) kernel at a particular
point in time, with a number of patches applied.

These patches typically provide features that, to a greater or lesser extent,
are deemed commercially desirable, and thus able to provide competitive
differentiation in the marketplace. In some cases, these patches are later
accepted into the mainline kernel. In fact, some new kernel features were
initially developed by a distribution company, and appeared in their
distribution before eventually being integrated into the mainline.

The upshot of the preceding points is that there are (mostly minor) differences
in the systems offered by the various Linux distribution companies. On a much
smaller scale, this is reminiscent of the splits in implementations that
occurred in the early years of UNIX. The Linux Standard Base (LSB) is an effort
to ensure compatibility among the various Linux distributions. To do this, the
LSB develops and promotes a set of standards for Linux systems with the aim of
ensuring that binary applications (i.e., compiled programs) can run on any
LSB-conformant system.

**Note:** The binary portability promoted by the LSB contrasts with the source
code portability promoted by POSIX. Source code portability means that we can
write a C program and then successfully compile and run it on any
POSIXconformant system. Binary compatibility is much more demanding and is
generally not feasible across different hardware platforms. It allows us to
compile a program once for a given hardware platform, and then run that compiled
program on any conformant implementation running on that hardware platform.
Binary portability is an essential requirement for the commercial viability of
independent software vendor (ISV) applications built for Linux.

### Summary

The UNIX system was first implemented in 1969 on a Digital PDP-7 minicomputer by
Ken Thompson at Bell Laboratories (part of AT&T). The operating system drew many
ideas, as well as its punned name, from the earlier MULTICS system. By 1973,
UNIX had been moved to the PDP-11 mini-computer and rewritten in C, a
programming language designed and implemented at Bell Laboratories by Dennis
Ritchie. Legally prevented from selling UNIX, AT&T instead distributed the
complete system to universities for a nominal charge. This distribution included
source code, and became very popular within universities, since it provided a
cheap operating system whose code could be studied and modified by computer
science academics and students.

The University of California at Berkeley played a key role in the development of
the UNIX system. There, Ken Thompson and a number of graduate students extended
the operating system. By 1979, the University was producing its own UNIX
distribution, BSD. This distribution became widespread in academia and formed
the basis for several commercial implementations.

Meanwhile, the breakup of the AT&T monopoly permitted the company to sell the
UNIX system. This resulted in the other major variant of UNIX, System V, which
also formed the basis for several commercial implementations.

Two different currents led to the development of (GNU/) Linux. One of these was
the GNU project, founded by Richard Stallman. By the late 1980s, the GNU project
had produced an almost complete, freely distributable UNIX implementation. The
one part lacking was a working kernel. In 1991, inspired by the Minix kernel
written by Andrew Tanenbaum, Linus Torvalds produced a working UNIX kernel for
the Intel x86-32 architecture. Torvalds invited other programmers to join him in
improving the kernel. Many programmers did so, and, over time, Linux was
extended and ported to a wide variety of hardware architectures.

The portability problems that arose from the variations in UNIX and C
implementations that existed by the late 1980s created a strong pressure for
standardization. The C language was standardized in 1989 (C89), and a revised
standard was produced in 1999 (C99). The first attempt to standardize the
operating system interface yielded POSIX.1, ratified as an IEEE standard in
1988, and as an ISO standard in 1990. During the 1990s, further standards were
drafted, including various versions of the Single UNIX Specification. In 2001,
the combined POSIX 1003.1-2001 and SUSv3 standard was ratified. This standard
consolidates and extends various earlier POSIX standards and earlier versions of
the Single UNIX Specification. In 2008, a less wide-ranging revision of the
standard was completed, yielding the combined POSIX 1003.1-2008 and SUSv4
standard.

Unlike most commercial UNIX implementations, Linux separates implementation from
distribution. Consequently, there is no single “official” Linux distribution.
Each Linux distributor’s offering consists of a snapshot of the current stable
kernel, with various patches applied. The LSB develops and promotes a set of
standards for Linux systems with the aim of ensuring binary application
compatibility across Linux distributions, so that compiled applications should
be able to run on any LSB-conformant system running on the same hardware.

## Fundamental Concepts

### The Core Operating System: The Kernel

The term _operating system_ is commonly used with two different meanings:

- To denote the entire package consisting of the central software managing a
  computer’s resources and all of the accompanying standard software tools, such
  as command-line interpreters, graphical user interfaces, file utilities, and
  editors.
- More narrowly, to refer to the central software that manages and allocates
  computer resources (i.e., the CPU, RAM, and devices).

The term _kernel_ is often used as a synonym for the second meaning.

The Linux kernel executable typically resides at the pathname `/boot/vmlinuz`.

#### Tasks performed by the kernel

- _Process sceduling_: A computer has one or more central processing units
  (CPUs), which execute the instructions of programs. Like other UNIX systems,
  Linux is a _preemptive multitasking_ operating system, _Multitasking_ means
  that multiple processes (i.e., running programs) can simultaneously reside in
  memory and each may receive use of the CPU(s). _Preemptive_ means that the
  rules governing which processes receive use of the CPU and for how long are
  determined by the kernel process scheduler (rather than by the processes
  themselves).

- _Memory management_: While computer memories are enormous by the standards of
  a decade or two ago, the size of software has also correspondingly grown, so
  that physical memory (RAM) remains a limited resource that the kernel must
  share among processes in an equitable and efficient fashion. Like most modern
  operating systems, Linux employs virtual memory management, a technique that
  confers two main advantages:

  - Processes are isolated from one another and from the kernel, so that one
    process can’t read or modify the memory of another process or the kernel.
  - Only part of a process needs to be kept in memory, thereby lowering the
    memory requirements of each process and allowing more processes to be held
    in RAM simultaneously. This leads to better CPU utilization, since it
    increases the likelihood that, at any moment in time, there is at least one
    process that the CPU(s) can execute.

- _Provision of a file system_: The kernel provides a file system on disk,
  allowing files to be created, retrieved, updated, deleted, and so on.

- _Creation and termination of processes_: The kernel can load a new program
  into memory, providing it with the resources (e.g., CPU, memory, and access to
  files) that it needs in order to run. Such an instance of a running program is
  termed a _process_. Once a process has completed execution, the kernel ensures
  that the resources it uses are freed for subsequent reuse by later programs.
- _Access to devices_: The devices (mice, monitors, keyboards, disk and tape
  drives, and so on) attached to a computer allow communication of information
  between the computer and the outside world, permitting input, output, or both.
  The kernel provides programs with an interface that standardizes and
  simplifies access to devices, while at the same time arbitrating access by
  multiple processes to each device.
- _Networking:_: The kernel transmits and receives network messages (packets) on
  behalf of user processes. This task includes routing of network packets to the
  target system.
- _Provision of a system call application programming interface (API)_:
  Processes can request the kernel to perform various tasks using kernel entry
  points known as _system calls_. The Linux system call API is the primary topic
  of this book.

In addition to the above features, multiuser operating systems such as Linux
generally provide users with the abstraction of a _virtual private computer_;
that is, each user can log on to the system and operate largely independently of
other users.

#### Kernel mode and user mode

Modern processor architectures typically allow the CPU to operate in at least
two different modes: _user mode_ and _kernel mode_ (sometimes also referred to
as _supervisor mode_). Hardware instructions allow switching from one mode to
the other. Correspondingly, areas of virtual memory can be marked as being part
of _user space_ or _kernel space_. When running in user mode, the CPU can access
only memory that is marked as being in user space; attempts to access memory in
kernel space result in a hardware exception. When running in kernel mode, the
CPU can access both user and kernel memory space.

Certain operations can be performed only while the processor is operating in
kernel mode. Examples include executing the halt instruction to stop the system,
accessing the memory-management hardware, and initiating device I/O operations.
By taking advantage of this hardware design to place the operating system in
kernel space, operating system implementers can ensure that user processes are
not able to access the instructions and data structures of the kernel, or to
perform operations that would adversely affect the operation of the system.

#### Process versus kernel views of the system

Later in this book we’ll say things such as “a process can create another
process,” “a process can create a pipe,” “a process can write data to a file,”
and “a process can terminate by calling `exit()`.” Remember, however, that the
kernel mediates all such actions, and these statements are just shorthand for “a
process can _request that the kernel_ create another process,” and so on.

### The Shell

## System Programming Concepts

## File I/O: The Universal I/O Model

## File I/O: Further Details

## Processes

## Memory Allocation

## Users and Groups

## Process Credentials

## Time

## System Limits and Options

## System and Process Information

## File I/O Buffering

## File Systems

## File Attributes

## Extended Attributes

## Access Control Lists

## Directories and Links

## Monitoring File Events

## Signals: Fundamental Concepts

## Signals: Signal Handlers

## Signals: Advanced Features

## Timers and Sleeping

## Process Creation

## Process Termination

## Monitoring Child Processes

## Program Execution

## Process Creation and Program Execution in More Detail

## Threads: Introduction

## Threads: Thread Synchronization

## Threads: Thread Safety and Per-Thread Storage

## Threads: Thread Cancellation

## Threads: Further Details

## Process Groups, Sessions, and Job Control

## Process Priorities and Scheduling

## Process Resources

## Daemons

## Writing Secure Privileged Programs

## Capabilities

## Login Accounting

## Fundamentals of Shared Libraries

## Advanced Features of Shared Libraries

## Interprocess Communication Overview

## Pipes and FIFOs

## Introduction to System V IPC

## System V Message Queues

## System V Semaphores

## System V Shared Memory

## Memory Mappings

## Virtual Memory Operations

## Introduction to POSIX IPC

## POSIX Message Queues

## POSIX Semaphores

## POSIX Shared Memory

## File Locking

## Sockets: Introduction

## Sockets: UNIX Domain

## Sockets: Fundamentals of TCP/IP Networks

## Sockets: Internet Domains

## Sockets: Server Design

## Sockets: Advanced Topics

## Terminals

## Alternative I/O Models

## Pseudoterminals
