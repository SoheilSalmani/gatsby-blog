---
title: Kubernetes in Action
resourceId: "9781617293726"
stoppedAt: "Pods: running containers in Kubernetes"
---

## Overview

### Introducing Kubernetes

#### Understanding the need for a system like Kubernetes

##### Moving from monolithic apps to microservices

Running a monolithic application usually requires a small number of powerful
servers that can provide enough resources for running the application. To deal
with increasing loads on the system, you then either have to vertically scale
the servers (also known as scaling up) by adding more CPUs, memory, and other
server components, or scale the whole system horizontally, by setting up
additional servers and running multiple copies (or replicas) of an application
(scaling out).

While scaling up usually doesn’t require any changes to the app, it gets
expensive relatively quickly and in practice always has an upper limit. Scaling
out, on the other hand, is relatively cheap hardware- wise, but may require big
changes in the application code and isn’t always possible—certain parts of an
application are extremely hard or next to impossible to scale horizontally
(relational databases, for example).

###### Splitting apps into microservices

<Figure src="/media/splitting-apps-into-microservices.png">
  Components inside a monolithic application vs. standalone microservices.
</Figure>

###### Scaling microservices

<Figure src="/media/scaling-microservices.png">
  Each microservice can be scaled individually.
</Figure>

#### Introducing container technologies

##### Understanding what containers are

###### Isolating components with Linux container technologies

A process running in a container runs inside the host’s operating system, like
all the other processes (unlike VMs, where processes run in separate operating
systems). But the process in the container is still isolated from other
processes. To the process itself, it looks like it’s the only one running on the
machine and in its operating system.

###### Comparing virtual machines to containers

<Figure src="/media/vms-vs-containers.png">
  Using VMs to isolate groups of applications vs. isolating individual apps with
  containers.
</Figure>

When you run three VMs on a host, you have three completely separate operating
systems running on and sharing the same bare-metal hardware. Underneath those
VMs is the host’s OS and a hypervisor, which divides the physical hardware
resources into smaller sets of virtual resources that can be used by the
operating system inside each VM. Applications running inside those VMs perform
system calls to the guest OS’ kernel in the VM, and the kernel then performs x86
instructions on the host’s physical CPU through the hypervisor.

**Note:** Two types of hypervisors exist. Type 1 hypervisors don’t use a host
OS, while Type 2 do.

Containers, on the other hand, all perform system calls on the exact same kernel
running in the host OS. This single kernel is the only one performing x86
instructions on the host’s CPU. The CPU doesn’t need to do any kind of
virtualization the way it does with VMs.

<Figure src="/media/vms-vs-containers-2.png">
  The difference between how apps in VMs use the CPU vs. how they use them in
  containers.
</Figure>

###### Introducing the mechanisms that make container isolation possible

Two mechanisms make isolation possible:

- _Linux Namespaces_, makes sure each process sees its own personal view of the
  system (files, processes, network interfaces, hostname, and so on).
- _Linux Control Groups (cgroups)_, which limit the amount of resources the
  process can consume (CPU, memory, network bandwidth, and so on).

###### Isolating processes with Linux namespaces

By default, each Linux system initially has one single namespace. All system
resources, such as filesystems, process IDs, user IDs, network interfaces, and
others, belong to the single namespace. But you can create additional namespaces
and organize resources across them. When running a process, you run it inside
one of those namespaces. The process will only see resources that are inside the
same namespace. Well, multiple kinds of namespaces exist, so a process doesn’t
belong to one namespace, but to one namespace of each kind.

The following kinds of namespaces exist:

- Mount (mnt)
- Process ID (pid)
- Network (net)
- Inter-process communication (ipc)
- UTS
- User ID (user)

Each namespace kind is used to isolate a certain group of resources.

##### Introducing the Docker container platform

For example, if you’ve packaged up your application with the files of the whole
Red Hat Enterprise Linux (RHEL) operating system, the application will believe
it’s running inside RHEL, both when you run it on your development computer that
runs Fedora and when you run it on a server running Debian or some other Linux
distribution. Only the kernel may be different.

A big difference between Docker-based container images and VM images is that
container images are composed of layers, which can be shared and reused across
multiple images. This means only certain layers of an image need to be
downloaded if the other layers were already downloaded previously when running a
different container image that also contains the same layers.

###### Understanding Docker concepts

Three main concepts in Docker comprise this scenario:

- _Images_—A Docker-based container image is something you package your
  application and its environment into. It contains the filesystem that will be
  available to the application and other metadata, such as the path to the
  executable that should be executed when the image is run.
- _Registries_—A Docker Registry is a repository that stores your Docker images
  and facilitates easy sharing of those images between different people and
  computers. When you build your image, you can either run it on the computer
  you’ve built it on, or you can _push_ (upload) the image to a registry and
  then _pull_ (download) it on another computer and run it there. Certain
  registries are public, allowing anyone to pull images from it, while others
  are private, only accessible to certain people or machines.
- _Containers_—A Docker-based container is a regular Linux container created
  from a Docker-based container image. A running container is a process running
  on the host running Docker, but it’s completely isolated from both the host
  and all other processes running on it. The process is also
  resource-constrained, meaning it can only access and use the amount of
  resources (CPU, RAM, and so on) that are allocated to it.

###### Building, distributing, and running a Docker image

<Figure src="/media/docker-images-registries-containers.png">
  Docker images, registries, and containers.
</Figure>

###### Comparing virtual machines and Docker containers

<Figure src="/media/6-apps-3-vms-docker-containers.png">
  Running six apps on three VMs vs. running them in Docker containers.
</Figure>

###### Understanding image layers

Layers don’t only make distribution more efficient, they also help reduce the
storage footprint of images. Each layer is only stored once. Two containers
created from two images based on the same base layers can therefore read the
same files, but if one of them writes over those files, the other one doesn’t
see those changes. Therefore, even if they share files, they’re still isolated
from each other. This works because container image layers are read-only. When a
container is run, a new writable layer is created on top of the layers in the
image. When the process in the container writes to a file located in one of the
underlying layers, a copy of the whole file is created in the top-most layer and
the process writes to the copy.

###### Understanding the portability limitations of container images

All containers running on a host use the host’s Linux kernel.

And it’s not only about the kernel. It should also be clear that a containerized
app built for a specific hardware architecture can only run on other machines
that have the same architecture. You can’t containerize an application built for
the x86 architecture and expect it to run on an ARM-based machine because it
also runs Docker. You still need a VM for that.

##### Introducing rkt—an alternative to Docker

After the success of Docker, the Open Container Initiative (OCI) was born to
create open industry standards around container formats and runtime. Docker is
part of that initiative, as is _rkt_ (pronounced “rock-it”), which is another
Linux container engine.

Like Docker, rkt is a platform for running containers. It puts a strong emphasis
on security, composability, and conforming to open standards. It uses the OCI
container image format and can even run regular Docker container images.

This book focuses on using Docker as the container runtime for Kubernetes,
because it was initially the only one supported by Kubernetes. Recently,
Kubernetes has also started supporting rkt, as well as others, as the container
runtime.

#### Introducing Kubernetes

##### Looking at Kubernetes from the top of a mountain

Kubernetes enables you to run your software applications on thousands of
computer nodes as if all those nodes were a single, enormous computer. It
abstracts away the underlying infrastructure and, by doing so, simplifies
development, deployment, and management for both development and the operations
teams.

###### Understanding the core of what kubernetes does

When the developer submits a list of apps to the master, Kubernetes deploys them
to the cluster of worker nodes. What node a component lands on doesn’t (and
shouldn’t) matter—neither to the developer nor to the system administrator.

<Figure src="/media/understanding-kubernetes.png">
  Kubernetes exposes the whole datacenter as a single deployment platform.
</Figure>

###### Helping developers focus on the core app features

Kubernetes can be thought of as an operating system for the cluster. It relieves
application developers from having to implement certain infrastructure-related
services into their apps; instead they rely on Kubernetes to provide these
services. This includes things such as service discovery, scaling,
load-balancing, self-healing, and even leader election.

###### Helping ops teams achieve better resource utilization

Because your application doesn’t care which node it’s running on, Kubernetes can
relocate the app at any time, and by mixing and matching apps, achieve far
better resource utilization than is possible with manual scheduling.

##### Understanding the architecture of a Kubernetes cluster

At the hardware level, a Kubernetes cluster is composed of many nodes, which can
be split into two types:

- The _master_ node, which hosts the _Kubernetes Control Plane_ that controls
  and manages the whole Kubernetes system
- Worker _nodes_ that run the actual applications you deploy

<Figure src="/media/kubernetes-cluster.png">
  The components that make up a Kubernetes cluster
</Figure>

###### The Control Plane

The Control Plane is what controls the cluster and makes it function. It
consists of multiple components that can run on a single master node or be split
across multiple nodes and replicated to ensure high availability. These
components are

- The _Kubernetes API Server_, which you and the other Control Plane components
  communicate with
- The _Scheduler_, which schedules your apps (assigns a worker node to each
  deployable component of your application)
- The _Controller Manager_, which performs cluster-level functions, such as
  replicating components, keeping track of worker nodes, handling node failures,
  and so on
- _etcd_, a reliable distributed data store that persistently stores the cluster
  configuration.

The components of the Control Plane hold and control the state of the cluster,
but they don’t run your applications. This is done by the (worker) nodes.

###### The nodes

The worker nodes are the machines that run your containerized applications. The
task of running, monitoring, and providing services to your applications is done
by the following components:

- Docker, rkt, or another _container runtime_, which runs your containers
- The _Kubelet_, which talks to the API server and manages containers on its
  node
- The _Kubernetes Service Proxy (kube-proxy)_, which load-balances network
  traffic between application components.

##### Running an application in Kubernetes

To run an application in Kubernetes, you first need to package it up into one or
more container images, push those images to an image registry, and then post a
description of your app to the Kubernetes API server.

The description includes information such as the container image or images that
contain your application components, how those components are related to each
other, and which ones need to be run co-located (together on the same node) and
which don’t. For each component, you can also specify how many copies (or
_replicas_) you want to run. Additionally, the description also includes which
of those components provide a service to either internal or external clients and
should be exposed through a single IP address and made discoverable to the other
components.

###### Understanding how the description results in a container

When the API server processes your app’s description, the Scheduler schedules
the specified groups of containers onto the available worker nodes based on
computational resources required by each group and the unallocated resources on
each node at that moment. The Kubelet on those nodes then instructs the
Container Runtime (Docker, for example) to pull the required container images
and run the containers.

<Figure src="/media/kubernetes-architecture-overview.png">
  A basic overview of the Kubernetes architecture and an application running on
  top of it.
</Figure>

The app descriptor lists four containers, grouped into three sets (these sets
are called _pods_). The first two pods each contain only a single container,
whereas the last one contains two. That means both containers need to run
co-located and shouldn’t be isolated from each other. Next to each pod, you also
see a number representing the number of replicas of each pod that need to run in
parallel. After submitting the descriptor to Kubernetes, it will schedule the
specified number of replicas of each pod to the available worker nodes. The
Kubelets on the nodes will then tell Docker to pull the container images from
the image registry and run the containers.

###### Keeping the containers running

Once the application is running, Kubernetes continuously makes sure that the
deployed state of the application always matches the description you provided.
For example, if you specify that you always want five instances of a web server
running, Kubernetes will always keep exactly five instances running. If one of
those instances stops working properly, like when its process crashes or when it
stops responding, Kubernetes will restart it automatically.

Similarly, if a whole worker node dies or becomes inaccessible, Kubernetes will
select new nodes for all the containers that were running on the node and run
them on the newly selected nodes.

###### Scaling the number of copies

While the application is running, you can decide you want to increase or
decrease the number of copies, and Kubernetes will spin up additional ones or
stop the excess ones, respectively. You can even leave the job of deciding the
optimal number of copies to Kubernetes. It can automatically keep adjusting the
number, based on real-time metrics, such as CPU load, memory consumption,
queries per second, or any other metric your app exposes.

###### Hitting a moving target

We’ve said that Kubernetes may need to move your containers around the cluster.
This can occur when the node they were running on has failed or because they
were evicted from a node to make room for other containers. If the container is
providing a service to external clients or other containers running in the
cluster, how can they use the container properly if it’s constantly moving
around the cluster? And how can clients connect to containers providing a
service when those containers are replicated and spread across the whole
cluster?

To allow clients to easily find containers that provide a specific service, you
can tell Kubernetes which containers provide the same service and Kubernetes
will expose all of them at a single static IP address and expose that address to
all applications running in the cluster. This is done through environment
variables, but clients can also look up the service IP through good old DNS. The
kube-proxy will make sure connections to the service are load balanced across
all the containers that provide the service. The IP address of the service stays
constant, so clients can always connect to its containers, even when they’re
moved around the cluster.

##### Understanding the benefits of using Kubernetes

If you have Kubernetes deployed on all your servers, the ops team doesn’t need
to deal with deploying your apps anymore. Because a containerized application
already contains all it needs to run, the system administrators don’t need to
install anything to deploy and run the app. On any node where Kubernetes is
deployed, Kubernetes can run the app immediately without any help from the
sysadmins.

###### Simplifying application deployment

In essence, all the nodes are now a single bunch of computational resources that
are waiting for applications to consume them. A developer doesn’t usually care
what kind of server the application is running on, as long as the server can
provide the application with adequate system resources.

Certain cases do exist where the developer does care what kind of hardware the
application should run on. If the nodes are heterogeneous, you’ll find cases
when you want certain apps to run on nodes with certain capabilities and run
other apps on others. For example, one of your apps may require being run on a
system with SSDs instead of HDDs, while other apps run fine on HDDs. In such
cases, you obviously want to ensure that particular app is always scheduled to a
node with an SSD.

Using Kubernetes, instead of selecting a specific node where your app should be
run, it’s more appropriate to tell Kubernetes to only choose among nodes with an
SSD.

###### Achieving better utilization of hardware

When you tell Kubernetes to run your application, you’re letting it choose the
most appropriate node to run your application on based on the description of the
application’s resource requirements and the available resources on each node.

By using containers and not tying the app down to a specific node in your
cluster, you’re allowing the app to freely move around the cluster at any time,
so the different app components running on the cluster can be mixed and matched
to be packed tightly onto the cluster nodes. This ensures the node’s hardware
resources are utilized as best as possible.

###### Health checking and self-healing

Having a system that allows moving an application across the cluster at any time
is also valuable in the event of server failures. As your cluster size
increases, you’ll deal with failing computer components ever more frequently.

Kubernetes monitors your app components and the nodes they run on and
automatically reschedules them to other nodes in the event of a node failure.
This frees the ops team from having to migrate app components manually and
allows the team to immediately focus on fixing the node itself and returning it
to the pool of available hardware resources instead of focusing on relocating
the app.

If your infrastructure has enough spare resources to allow normal system
operation even without the failed node, the ops team doesn’t even need to react
to the failure.

###### Automatic scaling

Using Kubernetes to manage your deployed applications also means the ops team
doesn’t need to constantly monitor the load of individual applications to react
to sudden load spikes. Kubernetes can be told to monitor the resources used by
each application and to keep adjusting the number of running instances of each
application.

If Kubernetes is running on cloud infrastructure, where adding additional nodes
is as easy as requesting them through the cloud provider’s API, Kubernetes can
even automatically scale the whole cluster size up or down based on the needs of
the deployed applications.

###### Simplifying application development

If you turn back to the fact that apps run in the same environment both during
development and in production, this has a big effect on when bugs are
discovered. We all agree the sooner you discover a bug, the easier it is to fix
it, and fixing it requires less work. It’s the developers who do the fixing, so
this means less work for them.

Then there’s the fact that developers don’t need to implement features that they
would usually implement. This includes discovery of services and/or peers in a
clustered application. Kubernetes does this instead of the app. Usually, the app
only needs to look up certain environment variables or perform a DNS lookup. If
that’s not enough, the application can query the Kubernetes API server directly
to get that and/or other information.

Querying the Kubernetes API server like that can even save developers from
having to implement complicated mechanisms such as leader election.

As a final example of what Kubernetes brings to the table, you also need to
consider the increase in confidence developers will feel knowing that when a new
version of their app is going to be rolled out, Kubernetes can automatically
detect if the new version is bad and stop its rollout immediately. This increase
in confidence usually accelerates the continuous delivery of apps, which
benefits the whole organization.

### First steps with Docker and Kubernetes

#### Creating, running, and sharing a container image

##### Installing Docker and running a Hello World container

```shell
docker run busybox echo "Hello world"
```

<Figure src="/media/running-busybox-container.png">
  Running echo “Hello world” in a container based on the busybox container
  image.
</Figure>

All software packages get updated, so more than a single version of a package
usually exists. Docker supports having multiple versions or variants of the same
image under the same name. Each variant must have a unique tag. When referring
to images without explicitly specifying the tag, Docker will assume you’re
referring to the so-called _latest_ tag.

```shell
docker run <image>:<tag>
```

##### Creating a trivial Node.js app

```js nu fp=app.js
const http = require("http")
const os = require("os")

console.log("Kubia server starting...")

var handler = function (request, response) {
  console.log("Received request from " + request.connection.remoteAddress)
  response.writeHead(200)
  response.end("You've hit " + os.hostname() + "\n")
}

var www = http.createServer(handler)
www.listen(8080)
```

##### Creating a Dockerfile for the image

```dockerfile
FROM node:7
ADD app.js /app.js
ENTRYPOINT ["node", "app.js"]
```

In the third line, you’re defining what command should be executed when somebody
runs the image.

##### Building the container image

```shell
docker build -t kubia .
```

<Figure src="/media/building-a-container-image-from-dockerfile.png">
  Building a new container image from a Dockerfile.
</Figure>

###### Understanding how an image is built

The build process isn’t performed by the Docker client. Instead, the contents of
the whole directory are uploaded to the Docker daemon and the image is built
there. The client and daemon don’t need to be on the same machine at all.
Because all the files in the build directory are uploaded to the daemon, if it
contains many large files and the daemon isn’t running locally, the upload may
take longer.

**Tip:** Don’t include any unnecessary files in the build directory, because
they’ll slow down the build process—especially when the Docker daemon is on a
remote machine.

###### Understanding image layers

Different images may share several layers, which makes storing and transferring
images much more efficient. For example, if you create multiple images based on
the same base image, all the layers comprising the base image will be stored
only once. Also, when pulling an image, Docker will download each layer
individually. Several layers may already be stored on your machine, so Docker
will only download those that aren’t.

You may think that each Dockerfile creates only a single new layer, but that’s
not the case. When building an image, a new layer is created for each individual
command in the Dockerfile. During the build of your image, after pulling all the
layers of the base image, Docker will create a new layer on top of them and add
the app.js file into it. Then it will create yet another layer that will specify
the command that should be executed when the image is run. This last layer will
then be tagged as `kubia:latest`.

List all locally stored images:

```shell
docker images
```

###### Comparing building images with a Dockerfile vs. manually

Dockerfiles are the usual way of building container images with Docker, but you
could also build the image manually by running a container from an existing
image, executing commands in the container, exiting the container, and
committing the final state as a new image. This is exactly what happens when you
build from a Dockerfile, but it’s performed automatically and is repeatable,
which allows you to make changes to the Dockerfile and rebuild the image any
time, without having to manually retype all the commands again.

##### Running the container image

```shell
docker run --name kubia-container -p 8080:8080 -d kubia
```

The container will be detached from the console (`-d` flag), which means it will
run in the background. Port 8080 on the local machine will be mapped to port
8080 inside the container (`-p 8080:8080` option).

###### Accessing your app

```shell
curl localhost:8080
```

**Note:** The hostname is the ID of the Docker container.

###### Listing all running containers

```shell
docker ps
```

For each container, Docker prints out its ID and name, the image used to run the
container, and the command that’s executing inside the container.

##### Exploring the inside of a running container

```shell
docker exec -it kubia-container bash
```

The `-it` option is shorthand for two options:

- `-i`, which makes sure STDIN is kept open. You need this for entering commands
  into the shell.
- `-t`, which allocates a pseudo terminal (TTY).

You need both if you want the use the shell like you’re used to. (If you leave
out the first one, you can’t type any commands, and if you leave out the second
one, the command prompt won’t be displayed and some commands will complain about
the `TERM` variable not being set.)

###### Exploring the container from within

```shell
ps aux
```

###### Understanding that processes in a container run in the host operating system

```shell
ps aux | grep app.js
```

The processes have different IDs inside the container vs. on the host. The
container is using its own PID Linux namespace and has a completely isolated
process tree, with its own sequence of numbers.

###### The container's filesystem is also isolated

Listing the contents of the root directory inside the container will only show
the files in the container and will include all the files that are in the image
plus any files that are created while the container is running (log files and
similar).

###### Stopping and removing a container

```shell
docker stop kubia-container
```

The container itself still exists and you can see it with `docker ps -a`. The
`-a` option prints out all the containers, those running and those that have
been stopped.

To remove a container:

```shell
docker rm kubia-container
```

This deletes the container. All its contents are removed and it can’t be started
again.

##### Pushing the image to an image registry

Docker Hub will allow you to push an image if the image’s repository name starts
with your Docker Hub ID. You create your Docker Hub ID by registering at
[http://hub.docker.com](http://hub.docker.com).

###### Tagging an image under an additional tag

```shell
docker tag kubia luksa/kubia
```

This doesn’t rename the tag; it creates an additional tag for the same image.

```shell
docker images | head
```

Both `kubia` and `luksa/kubia` point to the same image ID, so they’re in fact
one single image with two tags.

Before you can push the image to Docker Hub, you need to log in under your user
ID with the `docker login` command.

Push the image to Docker Hub:

```shell
docker push luksa/kubia
```

###### Running the image on a different machine

```shell
docker run -p 8080:8080 -d luksa/kubia
```

#### Setting up a Kubernetes cluster

##### Running a local single-node Kubernetes cluster with Minikube

Minikube is a tool that sets up a single-node cluster that’s great for both
testing Kubernetes and developing apps locally.

Install Minikube.

Starting a Kubernetes cluster with Minikube:

```shell
minikube start
```

To interact with Kubernetes, you also need the `kubectl` CLI client.

Checking to see the cluster is up and `kubectl` can talk to it:

```shell
kubectl cluster-info
```

**Tip:** You can run `minikube ssh` to log into the Minikube VM and explore it
from the inside.

Setting up a Google Cloud project and downloading the necessary client binaries:

1. Enable the Kubernetes Engine API.
2. Download and install Google Cloud SDK.
3. Install the `kubectl` command-line tool with
   `gcloud components install kubectl`.

Creating a Kubernetes cluster with three nodes:

```shell
gcloud container clusters create kubia --num-nodes 3 --machine-type f1-micro
```

##### Using a hosted Kubernetes cluster with Google Kubernetes Engine

If you want to explore a full-fledged multi-node Kubernetes cluster instead, you
can use a managed Google Kubernetes Engine (GKE) cluster. This way, you don’t
need to manually set up all the cluster nodes and networking, which is usually
too much for someone making their first steps with Kubernetes.

<Figure src="/media/gke-three-node-kubernetes-cluster.png">
  How you’re interacting with your three-node Kubernetes cluster.
</Figure>

Each node runs Docker, the Kubelet and the kube-proxy. You’ll interact with the
cluster through the `kubectl` command line client, which issues REST requests to
the Kubernetes API server running on the master node.

Checking if the cluster is up by listing cluster nodes:

```shell
kubectl get nodes
```

**Tip:** You can log into one of the nodes with `gcloud compute ssh <node-name>`
to explore what's running on the node.

Retrieving additional details of an object:

```shell
kubectl describe node gke-kubia-85f6-node-0rrx
```

You could also have performed a simple `kubectl describe node` without typing
the node’s name and it would print out a detailed description of all the nodes.

##### Setting up an alias and command-line completion for kubectl

Creating an alias:

```bash fp=/home/<user>/.bashrc
alias k=kubectl
```

**Note:** You may already have the `k` executable if you used `gcloud` to set up
the cluster.

#### Running your first app on Kubernetes

##### Deploying your Node.js app

Like pods and other Kubernetes resources, we create a replication controller by
posting a JSON or YAML descriptor to the Kubernetes REST API endpoint

Let's create a YAML file for our replication controller:

```yaml nu fp=kubia-rc.yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
  labels:
    run: kubia
spec:
  replicas: 1
  selector:
    run: kubia
  template:
    metadata:
      labels:
        run: kubia
    spec:
      containers:
        - name: kubia
          image: luksa/kubia:latest
          ports:
            - containerPort: 8080
              protocol: TCP
```

```shell
kubectl create -f kubia-rc.yaml
```

A ReplicationController called `kubia` has been created.

###### Introducing Pods

Kubernetes doesn't deal with individual containers directly. Instead, it uses
the concept fo multiple co-located containers. This group of containers is
called a Pod.

A pod is a group of one or more tightly related containers that will always run
together on the same worker node and in the same Linux namespace(s). Each pod is
like a separate logical machine with its own IP, hostname, processes, and so on,
running a single application. All the containers in a pod will appear to be
running on the same logical machine, whereas containers in other pods, even if
they’re running on the same worker node, will appear to be running on a
different one.

Pods are spread out across different worker nodes.

<Figure src="/media/containers-pods-and-worker-nodes.png">
  The relationship between containers, pods, and physical worker nodes.
</Figure>

###### Listing pods

You can’t list individual containers, since they’re not standalone Kubernetes
objects.

Listing pods:

```shell
kubectl get pods
```

To see more information about the pod, you can use:

```shell
kubectl describe pods <pod-name>
```

###### Understanding what happened behind the scenes

When you ran the `kubectl` command, it created a new ReplicationController
object in the cluster by sending a REST HTTP request to the Kubernetes API
server. The ReplicationController then created a new pod, which was then
scheduled to one of the worker nodes by the Scheduler. The Kubelet on that node
saw that the pod was scheduled to it and instructed Docker to pull the specified
image from the registry because the image wasn’t available locally. After
downloading the image, Docker created and ran the container.

**Definition:** The term scheduling means assigning the pod to a node. The pod
is run immediately, not at a time in the future as the term might lead you to
believe.

<Figure src="/media/containers-pods-and-worker-nodes.png">

Running the `luksa/kubia` container image in Kubernetes.

</Figure>

##### Accessing your web application

Each pod gets its own IP address, but this address is internal to the cluster
and isn’t accessible from outside of it. To make the pod accessible from the
outside, you’ll expose it through a Service object. You’ll create a special
service of type `LoadBalancer`, because if you create a regular service (a
`ClusterIP` service), like the pod, it would also only be accessible from inside
the cluster. By creating a `LoadBalancer`-type service, an external load
balancer will be created and you can connect to the pod through the load
balancer’s public IP.

###### Creating a Service object

To create the service, you’ll tell Kubernetes to expose the
ReplicationController you created earlier:

```shell
kubectl expose rc kubia --type=LoadBalancer --name kubia-http
```

**Note:** We’re using the abbreviation `rc` instead of `replicationcontroller`.
Most resource types have an abbreviation like this so you don’t have to type the
full name (for example, `po` for `pods`, `svc` for `services`, and so on).

###### Listing services

Listing services:

```shell
kubectl get services
```

Once the load balancer is up, the external IP address of the service should be
displayed.

**Note:** Minikube doesn’t support `LoadBalancer` services, so the service will
never get an external IP. But you can access the service anyway through its
external port.

###### Accessing your service through its external IP

You can now send requests to your pod through the service’s external IP and
port:

```shell
curl 104.155.74.57:8080
```

If you look closely, you’ll see that the app is reporting the name of the pod as
its hostname. As already mentioned, each pod behaves like a separate independent
machine with its own IP address and hostname. Even though the application is
running in the worker node’s operating system, to the app it appears as though
it’s running on a separate machine dedicated to the app itself—no other
processes are running alongside it.

**Tip:** When using Minikube, you can get the IP and port through which you can
access the service by running `minikube service kubia-http`.

##### The logical parts of your system

###### Understanding how the ReplicationController, the Pod, and the Service fit together

By running the `kubectl run` command you created a ReplicationController, and
this ReplicationController is what created the actual Pod object. To make that
pod accessible from outside the cluster, you told Kubernetes to expose all the
pods managed by that ReplicationController as a single Service.

<Figure src="/media/replicationcontroller-pod-and-service.png">
  Your system consists of a ReplicationController, a Pod, and a Service.
</Figure>

###### Understanding the pod and its container

The main and most important component in your system is the pod. It contains
only a single container, but generally a pod can contain as many containers as
you want. Inside the container is your Node.js process, which is bound to port
8080 and is waiting for HTTP requests. The pod has its own unique private IP
address and hostname.

###### Understanding the role of the ReplicationController

The next component is the `kubia` ReplicationController. It makes sure there’s
always exactly one instance of your pod running. Generally,
ReplicationControllers are used to replicate pods (that is, create multiple
copies of a pod) and keep them running. In your case, you didn’t specify how
many pod replicas you want, so the Replication- Controller created a single one.
If your pod were to disappear for any reason, the ReplicationController would
create a new pod to replace the missing one.

###### Understanding why you need a service

To understand why you need services, you need to learn a key detail about pods.
They’re ephemeral. A missing pod is replaced with a new one by the
ReplicationController. This new pod gets a different IP address from the pod
it’s replacing. This is where services come in—to solve the problem of
ever-changing pod IP addresses, as well as exposing multiple pods at a single
constant IP and port pair.

When a service is created, it gets a static IP, which never changes during the
lifetime of the service. Instead of connecting to pods directly, clients should
connect to the service through its constant IP address. The service makes sure
one of the pods receives the connection, regardless of where the pod is
currently running (and what its IP address is).

Services represent a static location for a group of one or more pods that all
provide the same service. Requests coming to the IP and port of the service will
be forwarded to the IP and port of one of the pods belonging to the service at
that moment.

##### Horizontally scaling the application

List ReplicationControllers:

```shell
kubectl get replicationcontrollers
```

You can get a list of all the possible object types by invoking `kubectl get`.

###### Increasing the desired replica count

```shell
kubectl scale rc kubia --replicas=3
```

###### Seeing the results of the scale-out

```shell
kubectl get rc
kubectl get pods
```

###### Seeing requests hit all three pods when hitting the service

```shell
curl 104.155.74.57:8080
curl 104.155.74.57:8080
curl 104.155.74.57:8080
curl 104.155.74.57:8080
```

###### Visualizing the new state of your system

<Figure src="/media/three-pods-exposed-through-a-single-service.png">
  Three instances of a pod managed by the same ReplicationController and exposed
  through a single service IP and port.
</Figure>

##### Examining what nodes your app is running on

In the Kubernetes world, what node a pod is running on isn’t that important, as
long as it gets scheduled to a node that can provide the CPU and memory the pod
needs to run properly.

###### Displaying the pod IP and the pod's node when listing pods

```shell
kubectl get pods -o wide
```

###### Inspecting other details of a pod with kubectl describe

You can also see the node by using the `kubectl describe` command, which shows
many other details of the pod:

```shell
kubectl describe pod kubia-hczji
```

##### Introducing the Kubernetes dashboard

###### Accessing the dashboard when running Kubernetes in GKE

If you’re using Google Kubernetes Engine, you can find out the URL of the
dashboard through the `kubectl cluster-info` command:

```shell
kubectl cluster-info | grep dashboard
```

If you open this URL in a browser, you’re presented with a username and password
prompt. You’ll find the username and password by running the following command:

```shell
gcloud container clusters describe kubia | grep -E "(username|password):"
```

###### Accessing the dashboard when using Minikube

```shell
minikube dashboard
```

## Core concepts

### Pods: running containers in Kubernetes

### Replication and other controllers: deploying managed pods

### Services: enabling clients to discover and talk to pods

### Volumes: attaching disk storage to containers

### ConfigMaps and Secrets: configuring applications

### Accessing pod metadata and other resources from applications

### Deployments: updating applications declaratively

### StatefulSets: deploying replicated stateful applications

## Beyond the basics

### Understanding Kubernetes internals

### Securing the Kubernetes API server

### Securing cluster nodes and the network

### Managing pods' computational resources

### Automatic scaling of pods and cluster nodes

### Advanced scheduling

### Best practices for developing apps

### Extending Kubernetes
